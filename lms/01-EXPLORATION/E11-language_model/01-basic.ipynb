{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language model 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 셰익스피어 대본 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T04:47:45.058266Z",
     "start_time": "2020-09-08T04:47:44.744584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-08 13:47:44--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 34.64.4.112, 34.64.4.80, 34.64.4.16, ...\n",
      "접속 storage.googleapis.com (storage.googleapis.com)|34.64.4.112|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt’\n",
      "\n",
      "shakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-09-08 13:47:44 (9.52 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T04:51:28.498191Z",
     "start_time": "2020-09-08T04:51:27.352730Z"
    }
   },
   "outputs": [],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T04:51:54.294330Z",
     "start_time": "2020-09-08T04:51:54.288901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "# 파일을 읽기모드로 열어 봅니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/AIFFEL_LSG/lms/01-EXPLORATION/E11-language_model/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "print(raw_corpus[:9])    # 앞에서부터 10라인만 화면에 출력해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T04:57:30.959225Z",
     "start_time": "2020-09-08T04:57:30.957024Z"
    }
   },
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 전처리??\n",
    "- 정규표현식을 이용한 corpus 생성\n",
    "- tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "- tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n",
    "- dataset을 얻음으로써 데이터 다듬기 과정은 끝났습니다. tf.data.Dataset에서 제공하는 shuffle(), batch() 등 다양한 데이터셋 관련 기능을 손쉽게 이용할 수 있게 되었군요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T05:05:06.976891Z",
     "start_time": "2020-09-08T05:05:06.974150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "# 공백, 화자가 표기된 거 숙청\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "<br/>\n",
    "\n",
    "1. Hi, my name is John. *(\"Hi,\" \"my\", …, \"john.\" 으로 분리됨) - 문장부호   \n",
    "\n",
    "2. First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자   \n",
    "\n",
    "3. He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자   \n",
    "\n",
    "==>\"1.\" 을 막기 위해 문장 부호 양쪽에 공백을 추가 할 거고요, \"2.\" 를 막기 위해 모든 문자들을 소문자로 변환할 겁니다. \"3.\"을 막기 위해 특수문자들은 모두 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T05:12:15.664197Z",
     "start_time": "2020-09-08T05:12:15.660626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 활용\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T05:36:44.000656Z",
     "start_time": "2020-09-08T05:36:43.757116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:23:44.439178Z",
     "start_time": "2020-09-08T06:23:44.007009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc27f1b1350>\n"
     ]
    }
   ],
   "source": [
    "#tf keras txt tokenizer\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:37:34.770844Z",
     "start_time": "2020-09-08T06:37:34.768446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:37:48.505587Z",
     "start_time": "2020-09-08T06:37:48.502132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:42:04.485859Z",
     "start_time": "2020-09-08T06:42:04.483299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:42:16.147837Z",
     "start_time": "2020-09-08T06:42:16.034649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:01:33.990301Z",
     "start_time": "2020-09-08T07:01:33.975950Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:35:27.410194Z",
     "start_time": "2020-09-08T07:35:24.031485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-1.06914937e-04,  2.34186475e-04, -1.72473970e-04, ...,\n",
       "         -2.52085665e-05,  2.23432464e-04,  5.32667946e-06],\n",
       "        [-3.16553487e-04,  5.50994184e-04, -1.39552489e-04, ...,\n",
       "         -1.00985504e-04, -7.96598688e-05,  3.14427743e-04],\n",
       "        [-5.04662341e-04,  5.97226724e-04,  2.80011591e-04, ...,\n",
       "          2.00174050e-04, -4.61975986e-04,  5.71438519e-04],\n",
       "        ...,\n",
       "        [ 2.30920385e-03, -3.11308337e-04, -1.72583770e-03, ...,\n",
       "          6.18050166e-04,  2.59085139e-03, -2.02116137e-03],\n",
       "        [ 2.86544091e-03, -5.05010074e-04, -1.96058024e-03, ...,\n",
       "          6.57608849e-04,  2.82739685e-03, -2.32655765e-03],\n",
       "        [ 3.36070545e-03, -6.85744686e-04, -2.16331915e-03, ...,\n",
       "          6.96013682e-04,  3.04960296e-03, -2.59194453e-03]],\n",
       "\n",
       "       [[-1.06914937e-04,  2.34186475e-04, -1.72473970e-04, ...,\n",
       "         -2.52085665e-05,  2.23432464e-04,  5.32667946e-06],\n",
       "        [-1.68128900e-04,  5.21462527e-04,  1.05718987e-04, ...,\n",
       "          1.25693186e-04,  6.57252094e-04,  1.43080551e-05],\n",
       "        [-2.61754467e-04,  7.27112521e-04,  1.87129175e-04, ...,\n",
       "          5.99001942e-05,  8.12507758e-04,  3.47125926e-04],\n",
       "        ...,\n",
       "        [ 5.07313805e-03, -8.03589646e-04, -2.24959198e-03, ...,\n",
       "          8.34882376e-04,  3.70876468e-03, -3.57209519e-03],\n",
       "        [ 5.23515278e-03, -9.60083562e-04, -2.36253976e-03, ...,\n",
       "          8.73632729e-04,  3.80490860e-03, -3.72863980e-03],\n",
       "        [ 5.36718033e-03, -1.09917985e-03, -2.46241968e-03, ...,\n",
       "          9.01321357e-04,  3.87934735e-03, -3.86194582e-03]],\n",
       "\n",
       "       [[-1.06914937e-04,  2.34186475e-04, -1.72473970e-04, ...,\n",
       "         -2.52085665e-05,  2.23432464e-04,  5.32667946e-06],\n",
       "        [-3.02250410e-05,  1.48985549e-04, -2.12572559e-04, ...,\n",
       "          2.21557610e-04,  3.73742398e-04, -1.95351604e-04],\n",
       "        [-1.35490982e-05,  3.69070796e-04, -9.15316850e-05, ...,\n",
       "          3.12122109e-04,  3.88339482e-04, -2.52248952e-04],\n",
       "        ...,\n",
       "        [ 4.38426994e-03, -9.92546673e-04, -2.56655994e-03, ...,\n",
       "          5.96045516e-04,  3.53806745e-03, -2.93395622e-03],\n",
       "        [ 4.64151707e-03, -1.13376044e-03, -2.64163432e-03, ...,\n",
       "          6.68883091e-04,  3.68100754e-03, -3.17609310e-03],\n",
       "        [ 4.85875597e-03, -1.25711225e-03, -2.70644971e-03, ...,\n",
       "          7.27209321e-04,  3.79352830e-03, -3.38500063e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.06914937e-04,  2.34186475e-04, -1.72473970e-04, ...,\n",
       "         -2.52085665e-05,  2.23432464e-04,  5.32667946e-06],\n",
       "        [-1.98732887e-04,  1.60975527e-04, -5.38722728e-04, ...,\n",
       "         -3.27208574e-04,  7.63298740e-05,  2.13364518e-04],\n",
       "        [-1.91460727e-04,  2.29943951e-04, -7.33061985e-04, ...,\n",
       "         -5.75076498e-04,  2.16386718e-04,  7.41967466e-04],\n",
       "        ...,\n",
       "        [ 2.83032027e-03, -1.24474312e-03, -2.24217959e-03, ...,\n",
       "          4.09542001e-04,  2.42824340e-03, -1.51907897e-03],\n",
       "        [ 3.34759592e-03, -1.26950827e-03, -2.43427674e-03, ...,\n",
       "          4.13453381e-04,  2.70063034e-03, -1.89452874e-03],\n",
       "        [ 3.79154691e-03, -1.30699272e-03, -2.58904160e-03, ...,\n",
       "          4.36917297e-04,  2.94171250e-03, -2.23051896e-03]],\n",
       "\n",
       "       [[-1.06914937e-04,  2.34186475e-04, -1.72473970e-04, ...,\n",
       "         -2.52085665e-05,  2.23432464e-04,  5.32667946e-06],\n",
       "        [-1.67313585e-04,  3.46621178e-04, -2.41567774e-04, ...,\n",
       "         -9.38162411e-05,  1.84837583e-04, -5.18378045e-04],\n",
       "        [-2.97221442e-04,  2.31931743e-04, -2.24125019e-04, ...,\n",
       "         -9.94521251e-05,  1.12700938e-04, -8.62876128e-04],\n",
       "        ...,\n",
       "        [ 1.38856086e-03, -5.02997253e-04, -1.71950646e-03, ...,\n",
       "          3.66476597e-05,  2.15669116e-03, -1.10191596e-03],\n",
       "        [ 2.08413810e-03, -5.92432742e-04, -1.97846838e-03, ...,\n",
       "          1.17215641e-04,  2.43002083e-03, -1.54541153e-03],\n",
       "        [ 2.71037500e-03, -6.95256633e-04, -2.19383766e-03, ...,\n",
       "          2.07310120e-04,  2.69257231e-03, -1.94695999e-03]],\n",
       "\n",
       "       [[-1.06914937e-04,  2.34186475e-04, -1.72473970e-04, ...,\n",
       "         -2.52085665e-05,  2.23432464e-04,  5.32667946e-06],\n",
       "        [-4.20351949e-04,  9.97192983e-05, -6.02242653e-06, ...,\n",
       "         -1.09923676e-04,  3.51658760e-04,  1.06075160e-04],\n",
       "        [-2.25479685e-04, -3.14114586e-04,  2.80674518e-04, ...,\n",
       "          8.34691382e-05,  4.41025099e-04,  1.28819447e-04],\n",
       "        ...,\n",
       "        [ 2.61288066e-03,  2.12079482e-04, -1.71579327e-03, ...,\n",
       "         -6.65311876e-04,  2.03453051e-03, -1.37871911e-03],\n",
       "        [ 3.12110758e-03, -4.50631815e-05, -1.84787076e-03, ...,\n",
       "         -4.73310065e-04,  2.42975354e-03, -1.82563614e-03],\n",
       "        [ 3.56801413e-03, -2.88783864e-04, -1.96897076e-03, ...,\n",
       "         -2.76084873e-04,  2.78126821e-03, -2.21735844e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T01:16:07.981994Z",
     "start_time": "2020-09-10T01:16:07.946583Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:47:43.758265Z",
     "start_time": "2020-09-08T07:47:43.756853Z"
    }
   },
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:48:00.217271Z",
     "start_time": "2020-09-08T07:48:00.212635Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:48:20.638502Z",
     "start_time": "2020-09-08T07:48:20.531192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he annoy hoping stern easier jades deadly drugs poverty poverty poverty poverty boding boding unstanched kindle stripes stripes stripes '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
