{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Anomaly Detection with GAN\n",
    "\n",
    "- Anomaly Detection\n",
    "\n",
    "- 2017년에 발표된 Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery라는 논문에서 AnoGAN이라는 모델\n",
    "\n",
    "- https://arxiv.org/pdf/1703.05921.pdf\n",
    "\n",
    "- 현재 경로 이미지 참고..... 이론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:39:46.076895Z",
     "start_time": "2020-11-18T05:39:45.881492Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:39:11.940936Z",
     "start_time": "2020-11-18T05:39:10.330865Z"
    }
   },
   "source": [
    "# data set import(mnist)\n",
    "\n",
    "- Fashion-MNIST는 1채널 grayscale 데이터셋입니다. 그냥은 Convolution 연산이 되지 않으므로 채널방향 차원이 하나 늘어나도록 reshape하는 과정이 필요합니다. (주의) UNet 구조의 활용을 위해서 기존의 28 X 28 사이즈의 Fashion-MNIST 데이터 이미지를 32 X 32 로 패딩처리해 줄 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:40:05.987316Z",
     "start_time": "2020-11-18T05:40:02.175679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "(60000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Fashion MNIST padding to 32 X 32\n",
    "train_data_32 = np.zeros((train_data.shape[0], 32, 32)).astype('float32')\n",
    "test_data_32 = np.zeros((test_data.shape[0], 32, 32)).astype('float32')     \n",
    "train_data_32[:, 2:30, 2:30] = train_data\n",
    "test_data_32[:, 2:30, 2:30] = test_data\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data_32.reshape(train_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "test_data = test_data_32.reshape(test_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:42:58.584819Z",
     "start_time": "2020-11-18T05:42:58.244492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## 8번 라벨 bag 임의 로 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:44:11.088309Z",
     "start_time": "2020-11-18T05:44:11.084857Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:44:16.031183Z",
     "start_time": "2020-11-18T05:44:15.664038Z"
    }
   },
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:44:21.582650Z",
     "start_time": "2020-11-18T05:44:21.308685Z"
    }
   },
   "outputs": [],
   "source": [
    "# label 0을 anmaly data로\n",
    "# 나머지는 nomaly data 로분류\n",
    "\n",
    "\n",
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:46:05.307921Z",
     "start_time": "2020-11-18T05:46:05.304253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1) (54000, 1)\n",
      "(6000, 32, 32, 1) (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:00:01.398957Z",
     "start_time": "2020-11-18T06:00:01.370353Z"
    }
   },
   "outputs": [],
   "source": [
    "#train data / label은 0말 고 나머지\n",
    "#test data /label은 0인 넘들.....\n",
    "\n",
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:00:06.701609Z",
     "start_time": "2020-11-18T06:00:06.698427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1)\n",
      "(22000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:00:09.620786Z",
     "start_time": "2020-11-18T06:00:09.617606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 1)\n",
      "(22000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:00:39.998390Z",
     "start_time": "2020-11-18T06:00:39.938318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# train 라벨이 0 있는 지 혹시나 체크\n",
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:47:29.189175Z",
     "start_time": "2020-11-18T05:47:28.867364Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# from_tensor_slices tensor data set<으로\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "# \n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:47:34.997511Z",
     "start_time": "2020-11-18T05:47:34.571773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T05:47:58.004548Z",
     "start_time": "2020-11-18T05:47:57.902653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:32:09.582815Z",
     "start_time": "2020-11-18T06:32:09.576830Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:32:19.363248Z",
     "start_time": "2020-11-18T06:32:19.358853Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:34:01.315100Z",
     "start_time": "2020-11-18T06:34:01.308631Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## disciminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:34:25.781834Z",
     "start_time": "2020-11-18T06:34:25.776655Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:34:22.894391Z",
     "start_time": "2020-11-18T06:34:22.435270Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=1)  # Generator가 32X32X1 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:34:45.896538Z",
     "start_time": "2020-11-18T06:34:45.894019Z"
    }
   },
   "source": [
    "## loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:34:50.255833Z",
     "start_time": "2020-11-18T06:34:50.252840Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:34:54.857479Z",
     "start_time": "2020-11-18T06:34:54.854543Z"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:35:07.742912Z",
     "start_time": "2020-11-18T06:35:07.739520Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optiminzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T06:35:19.884364Z",
     "start_time": "2020-11-18T06:35:19.881153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:58:06.376704Z",
     "start_time": "2020-11-18T07:58:06.372297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:58:10.784189Z",
     "start_time": "2020-11-18T07:58:10.779402Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:58:11.414784Z",
     "start_time": "2020-11-18T07:58:11.411573Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'lsg/data/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-18T07:58:12.503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 30.974559783935547, \t Total Dis Loss : 0.0007348734070546925\n",
      "Steps : 200, \t Total Gen Loss : 30.26750373840332, \t Total Dis Loss : 0.003238727105781436\n",
      "Steps : 300, \t Total Gen Loss : 30.70307159423828, \t Total Dis Loss : 0.0015529505908489227\n",
      "Steps : 400, \t Total Gen Loss : 31.103967666625977, \t Total Dis Loss : 0.0013490122510120273\n",
      "Steps : 500, \t Total Gen Loss : 31.17817497253418, \t Total Dis Loss : 0.0005106075550429523\n",
      "Steps : 600, \t Total Gen Loss : 32.68364334106445, \t Total Dis Loss : 0.000861518201418221\n",
      "Steps : 700, \t Total Gen Loss : 28.976654052734375, \t Total Dis Loss : 0.0005386686534620821\n",
      "Steps : 800, \t Total Gen Loss : 32.363807678222656, \t Total Dis Loss : 0.000577768194489181\n",
      "Steps : 900, \t Total Gen Loss : 32.450050354003906, \t Total Dis Loss : 0.0018523933831602335\n",
      "Steps : 1000, \t Total Gen Loss : 34.002601623535156, \t Total Dis Loss : 0.0011352825677022338\n",
      "Steps : 1100, \t Total Gen Loss : 33.09938049316406, \t Total Dis Loss : 0.0007233654032461345\n",
      "Steps : 1200, \t Total Gen Loss : 32.711158752441406, \t Total Dis Loss : 0.0003567736712284386\n",
      "Steps : 1300, \t Total Gen Loss : 31.331785202026367, \t Total Dis Loss : 0.0008678056765347719\n",
      "Steps : 1400, \t Total Gen Loss : 31.04650115966797, \t Total Dis Loss : 0.0019813617691397667\n",
      "Steps : 1500, \t Total Gen Loss : 32.42752456665039, \t Total Dis Loss : 0.00047225889284163713\n",
      "Steps : 1600, \t Total Gen Loss : 31.330718994140625, \t Total Dis Loss : 0.000612677657045424\n",
      "Steps : 1700, \t Total Gen Loss : 29.36248207092285, \t Total Dis Loss : 0.0024337235372513533\n",
      "Steps : 1800, \t Total Gen Loss : 31.230815887451172, \t Total Dis Loss : 0.0005468247109092772\n",
      "Steps : 1900, \t Total Gen Loss : 31.345367431640625, \t Total Dis Loss : 0.0015375374350696802\n",
      "Steps : 2000, \t Total Gen Loss : 32.1650390625, \t Total Dis Loss : 0.0006745808059349656\n",
      "Steps : 2100, \t Total Gen Loss : 34.97509765625, \t Total Dis Loss : 0.0004022014036308974\n",
      "Steps : 2200, \t Total Gen Loss : 33.333133697509766, \t Total Dis Loss : 0.0004915869794785976\n",
      "Steps : 2300, \t Total Gen Loss : 30.80706787109375, \t Total Dis Loss : 0.0013000222388654947\n",
      "Steps : 2400, \t Total Gen Loss : 32.033817291259766, \t Total Dis Loss : 0.015764785930514336\n",
      "Steps : 2500, \t Total Gen Loss : 26.527681350708008, \t Total Dis Loss : 0.021419882774353027\n",
      "Steps : 2600, \t Total Gen Loss : 32.24144744873047, \t Total Dis Loss : 0.0012494472321122885\n",
      "Steps : 2700, \t Total Gen Loss : 31.087034225463867, \t Total Dis Loss : 0.0009264569962397218\n",
      "Steps : 2800, \t Total Gen Loss : 36.01791763305664, \t Total Dis Loss : 0.0005931394989602268\n",
      "Steps : 2900, \t Total Gen Loss : 33.66971206665039, \t Total Dis Loss : 0.0007912835571914911\n",
      "Steps : 3000, \t Total Gen Loss : 34.143272399902344, \t Total Dis Loss : 0.0001930318248923868\n",
      "Steps : 3100, \t Total Gen Loss : 32.82537841796875, \t Total Dis Loss : 0.0005119459819979966\n",
      "Steps : 3200, \t Total Gen Loss : 33.064109802246094, \t Total Dis Loss : 0.0004488386621233076\n",
      "Steps : 3300, \t Total Gen Loss : 32.577796936035156, \t Total Dis Loss : 0.00020290208340156823\n",
      "Steps : 3400, \t Total Gen Loss : 36.715431213378906, \t Total Dis Loss : 0.004842652939260006\n",
      "Steps : 3500, \t Total Gen Loss : 29.568981170654297, \t Total Dis Loss : 0.0049075777642428875\n",
      "Steps : 3600, \t Total Gen Loss : 29.729961395263672, \t Total Dis Loss : 0.009547569788992405\n",
      "Steps : 3700, \t Total Gen Loss : 31.5201416015625, \t Total Dis Loss : 0.0012391655473038554\n",
      "Steps : 3800, \t Total Gen Loss : 30.668319702148438, \t Total Dis Loss : 0.001060411799699068\n",
      "Steps : 3900, \t Total Gen Loss : 33.92347717285156, \t Total Dis Loss : 0.0010400566970929503\n",
      "Steps : 4000, \t Total Gen Loss : 31.091978073120117, \t Total Dis Loss : 0.014637940563261509\n",
      "Steps : 4100, \t Total Gen Loss : 31.410982131958008, \t Total Dis Loss : 0.0010834300192072988\n",
      "Steps : 4200, \t Total Gen Loss : 29.680538177490234, \t Total Dis Loss : 0.0019199445378035307\n",
      "Steps : 4300, \t Total Gen Loss : 32.64564514160156, \t Total Dis Loss : 0.0006730000022798777\n",
      "Steps : 4400, \t Total Gen Loss : 31.57139778137207, \t Total Dis Loss : 0.0008010531309992075\n",
      "Steps : 4500, \t Total Gen Loss : 35.409793853759766, \t Total Dis Loss : 0.004299853928387165\n",
      "Steps : 4600, \t Total Gen Loss : 31.93891716003418, \t Total Dis Loss : 0.001090470002964139\n",
      "Steps : 4700, \t Total Gen Loss : 31.541006088256836, \t Total Dis Loss : 0.0007973393658176064\n",
      "Steps : 4800, \t Total Gen Loss : 31.711490631103516, \t Total Dis Loss : 0.0005321560893207788\n",
      "Steps : 4900, \t Total Gen Loss : 32.32450485229492, \t Total Dis Loss : 0.00025600078515708447\n",
      "Steps : 5000, \t Total Gen Loss : 32.23008728027344, \t Total Dis Loss : 0.00021073970128782094\n",
      "Steps : 5100, \t Total Gen Loss : 30.987470626831055, \t Total Dis Loss : 0.0004586217401083559\n",
      "Steps : 5200, \t Total Gen Loss : 30.70499038696289, \t Total Dis Loss : 0.0008437715005129576\n",
      "Steps : 5300, \t Total Gen Loss : 33.116573333740234, \t Total Dis Loss : 0.00034893632982857525\n",
      "Steps : 5400, \t Total Gen Loss : 31.80466079711914, \t Total Dis Loss : 0.003961821552366018\n",
      "Steps : 5500, \t Total Gen Loss : 33.5962028503418, \t Total Dis Loss : 0.0004914934397675097\n",
      "Steps : 5600, \t Total Gen Loss : 35.17662048339844, \t Total Dis Loss : 0.00013982396922074258\n",
      "Steps : 5700, \t Total Gen Loss : 30.7631893157959, \t Total Dis Loss : 0.0003160088963340968\n",
      "Steps : 5800, \t Total Gen Loss : 32.74665451049805, \t Total Dis Loss : 0.0007407115772366524\n",
      "Steps : 5900, \t Total Gen Loss : 31.85846519470215, \t Total Dis Loss : 0.0004790876992046833\n",
      "Steps : 6000, \t Total Gen Loss : 31.10515022277832, \t Total Dis Loss : 0.000666739244479686\n",
      "Steps : 6100, \t Total Gen Loss : 32.527801513671875, \t Total Dis Loss : 0.00044004374649375677\n",
      "Steps : 6200, \t Total Gen Loss : 32.35962677001953, \t Total Dis Loss : 0.0013242991408333182\n",
      "Steps : 6300, \t Total Gen Loss : 32.13817596435547, \t Total Dis Loss : 0.00257851742208004\n",
      "Steps : 6400, \t Total Gen Loss : 32.258399963378906, \t Total Dis Loss : 0.0002574785612523556\n",
      "Steps : 6500, \t Total Gen Loss : 32.32449722290039, \t Total Dis Loss : 0.00030675315065309405\n",
      "Steps : 6600, \t Total Gen Loss : 31.067502975463867, \t Total Dis Loss : 0.0004876056045759469\n",
      "Steps : 6700, \t Total Gen Loss : 33.56719207763672, \t Total Dis Loss : 0.0014553576475009322\n",
      "Time for epoch 1 is 542.8464963436127 sec\n",
      "Steps : 6800, \t Total Gen Loss : 30.83877944946289, \t Total Dis Loss : 0.0005925598670728505\n",
      "Steps : 6900, \t Total Gen Loss : 29.053985595703125, \t Total Dis Loss : 0.00042544695315882564\n",
      "Steps : 7000, \t Total Gen Loss : 30.771808624267578, \t Total Dis Loss : 0.001964728580787778\n",
      "Steps : 7100, \t Total Gen Loss : 32.85186767578125, \t Total Dis Loss : 0.00048600733862258494\n",
      "Steps : 7200, \t Total Gen Loss : 31.436925888061523, \t Total Dis Loss : 0.000647944281809032\n",
      "Steps : 7300, \t Total Gen Loss : 33.484832763671875, \t Total Dis Loss : 0.004575956147164106\n",
      "Steps : 7400, \t Total Gen Loss : 31.438488006591797, \t Total Dis Loss : 0.0002205702621722594\n",
      "Steps : 7500, \t Total Gen Loss : 34.08899688720703, \t Total Dis Loss : 0.0005919878603890538\n",
      "Steps : 7600, \t Total Gen Loss : 33.29747009277344, \t Total Dis Loss : 0.002326709683984518\n",
      "Steps : 7700, \t Total Gen Loss : 31.78114128112793, \t Total Dis Loss : 0.0022390109952539206\n",
      "Steps : 7800, \t Total Gen Loss : 32.64869689941406, \t Total Dis Loss : 0.0006334354402497411\n",
      "Steps : 7900, \t Total Gen Loss : 30.83439064025879, \t Total Dis Loss : 0.0007247120956890285\n",
      "Steps : 8000, \t Total Gen Loss : 32.754024505615234, \t Total Dis Loss : 0.0019947218243032694\n",
      "Steps : 8100, \t Total Gen Loss : 32.00113296508789, \t Total Dis Loss : 0.0007600403041578829\n",
      "Steps : 8200, \t Total Gen Loss : 31.593704223632812, \t Total Dis Loss : 0.0015530576929450035\n",
      "Steps : 8300, \t Total Gen Loss : 32.30344009399414, \t Total Dis Loss : 0.00656677782535553\n",
      "Steps : 8400, \t Total Gen Loss : 31.462860107421875, \t Total Dis Loss : 0.00043166830437257886\n",
      "Steps : 8500, \t Total Gen Loss : 33.70134735107422, \t Total Dis Loss : 0.00028977420879527926\n",
      "Steps : 8600, \t Total Gen Loss : 32.644466400146484, \t Total Dis Loss : 0.0006312079494819045\n",
      "Steps : 8700, \t Total Gen Loss : 32.199974060058594, \t Total Dis Loss : 0.0010621342808008194\n",
      "Steps : 8800, \t Total Gen Loss : 31.813261032104492, \t Total Dis Loss : 0.0011499007232487202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8900, \t Total Gen Loss : 31.539316177368164, \t Total Dis Loss : 0.00034143580705858767\n",
      "Steps : 9000, \t Total Gen Loss : 35.41230392456055, \t Total Dis Loss : 0.000753417203668505\n",
      "Steps : 9100, \t Total Gen Loss : 30.61956024169922, \t Total Dis Loss : 0.0009030551882460713\n",
      "Steps : 9200, \t Total Gen Loss : 33.364959716796875, \t Total Dis Loss : 0.0005611955421045423\n",
      "Steps : 9300, \t Total Gen Loss : 31.718870162963867, \t Total Dis Loss : 0.00017349651898257434\n",
      "Steps : 9400, \t Total Gen Loss : 32.96361541748047, \t Total Dis Loss : 0.0005632448010146618\n",
      "Steps : 9500, \t Total Gen Loss : 31.46816635131836, \t Total Dis Loss : 0.0005479869432747364\n",
      "Steps : 9600, \t Total Gen Loss : 30.694217681884766, \t Total Dis Loss : 0.0004528373247012496\n",
      "Steps : 9700, \t Total Gen Loss : 30.940595626831055, \t Total Dis Loss : 0.0005589798092842102\n",
      "Steps : 9800, \t Total Gen Loss : 32.44208908081055, \t Total Dis Loss : 0.000742570380680263\n",
      "Steps : 9900, \t Total Gen Loss : 32.78911209106445, \t Total Dis Loss : 0.000252141326200217\n",
      "Steps : 10000, \t Total Gen Loss : 33.335819244384766, \t Total Dis Loss : 0.00032245280453935266\n",
      "Steps : 10100, \t Total Gen Loss : 34.77037048339844, \t Total Dis Loss : 0.003680250607430935\n",
      "Steps : 10200, \t Total Gen Loss : 32.037635803222656, \t Total Dis Loss : 0.00024752237368375063\n",
      "Steps : 10300, \t Total Gen Loss : 30.944040298461914, \t Total Dis Loss : 0.00048492284258827567\n",
      "Steps : 10400, \t Total Gen Loss : 30.991811752319336, \t Total Dis Loss : 0.0037283406127244234\n",
      "Steps : 10500, \t Total Gen Loss : 30.707590103149414, \t Total Dis Loss : 0.0014075018698349595\n",
      "Steps : 10600, \t Total Gen Loss : 30.016284942626953, \t Total Dis Loss : 0.002945313695818186\n",
      "Steps : 10700, \t Total Gen Loss : 31.614412307739258, \t Total Dis Loss : 0.0005900969845242798\n",
      "Steps : 10800, \t Total Gen Loss : 33.10747528076172, \t Total Dis Loss : 0.00031713530188426375\n",
      "Steps : 10900, \t Total Gen Loss : 32.69660186767578, \t Total Dis Loss : 0.0005063287680968642\n",
      "Steps : 11000, \t Total Gen Loss : 31.568971633911133, \t Total Dis Loss : 0.0007757422281429172\n",
      "Steps : 11100, \t Total Gen Loss : 34.700439453125, \t Total Dis Loss : 0.003453674726188183\n",
      "Steps : 11200, \t Total Gen Loss : 31.365388870239258, \t Total Dis Loss : 0.002460119780153036\n",
      "Steps : 11300, \t Total Gen Loss : 29.295740127563477, \t Total Dis Loss : 0.0005001571262255311\n",
      "Steps : 11400, \t Total Gen Loss : 31.3507137298584, \t Total Dis Loss : 0.0007697776891291142\n",
      "Steps : 11500, \t Total Gen Loss : 28.345577239990234, \t Total Dis Loss : 0.0038219525013118982\n",
      "Steps : 11600, \t Total Gen Loss : 32.61440658569336, \t Total Dis Loss : 0.0003151705313939601\n",
      "Steps : 11700, \t Total Gen Loss : 32.85334014892578, \t Total Dis Loss : 0.00020208890782669187\n",
      "Steps : 11800, \t Total Gen Loss : 28.28763198852539, \t Total Dis Loss : 0.0033283864613622427\n",
      "Steps : 11900, \t Total Gen Loss : 33.02180480957031, \t Total Dis Loss : 0.00024255829339381307\n",
      "Steps : 12000, \t Total Gen Loss : 32.06480407714844, \t Total Dis Loss : 0.0006118431920185685\n",
      "Steps : 12100, \t Total Gen Loss : 33.46611404418945, \t Total Dis Loss : 0.0007699484704062343\n",
      "Steps : 12200, \t Total Gen Loss : 32.6617317199707, \t Total Dis Loss : 0.0003099747118540108\n",
      "Steps : 12300, \t Total Gen Loss : 30.420686721801758, \t Total Dis Loss : 0.0014213406248018146\n",
      "Steps : 12400, \t Total Gen Loss : 31.82123374938965, \t Total Dis Loss : 0.0010596306528896093\n",
      "Steps : 12500, \t Total Gen Loss : 31.9195613861084, \t Total Dis Loss : 0.0016317700501531363\n",
      "Steps : 12600, \t Total Gen Loss : 31.716461181640625, \t Total Dis Loss : 0.00032459970680065453\n",
      "Steps : 12700, \t Total Gen Loss : 31.37613868713379, \t Total Dis Loss : 0.001812377362512052\n",
      "Steps : 12800, \t Total Gen Loss : 31.376218795776367, \t Total Dis Loss : 0.0008694280404597521\n",
      "Steps : 12900, \t Total Gen Loss : 30.168458938598633, \t Total Dis Loss : 0.000718772760592401\n",
      "Steps : 13000, \t Total Gen Loss : 34.913597106933594, \t Total Dis Loss : 0.0005568729247897863\n",
      "Steps : 13100, \t Total Gen Loss : 32.266605377197266, \t Total Dis Loss : 0.0008367725531570613\n",
      "Steps : 13200, \t Total Gen Loss : 32.436729431152344, \t Total Dis Loss : 0.0009255862678401172\n",
      "Steps : 13300, \t Total Gen Loss : 33.43566131591797, \t Total Dis Loss : 0.00028196442872285843\n",
      "Steps : 13400, \t Total Gen Loss : 33.6656608581543, \t Total Dis Loss : 0.043956249952316284\n",
      "Steps : 13500, \t Total Gen Loss : 33.512428283691406, \t Total Dis Loss : 0.00022353866370394826\n",
      "Time for epoch 2 is 540.2418389320374 sec\n",
      "Steps : 13600, \t Total Gen Loss : 33.8209114074707, \t Total Dis Loss : 0.0002589237701613456\n",
      "Steps : 13700, \t Total Gen Loss : 32.25047302246094, \t Total Dis Loss : 0.00023938732920214534\n",
      "Steps : 13800, \t Total Gen Loss : 33.1401481628418, \t Total Dis Loss : 0.0003036155831068754\n",
      "Steps : 13900, \t Total Gen Loss : 33.51285171508789, \t Total Dis Loss : 0.00013550228322856128\n",
      "Steps : 14000, \t Total Gen Loss : 32.0544548034668, \t Total Dis Loss : 0.0006373737705871463\n",
      "Steps : 14100, \t Total Gen Loss : 34.73792266845703, \t Total Dis Loss : 0.0012658473569899797\n",
      "Steps : 14200, \t Total Gen Loss : 33.59518814086914, \t Total Dis Loss : 0.00035220629069954157\n",
      "Steps : 14300, \t Total Gen Loss : 32.518470764160156, \t Total Dis Loss : 0.000901808962225914\n",
      "Steps : 14400, \t Total Gen Loss : 31.71424674987793, \t Total Dis Loss : 0.0005666235229000449\n",
      "Steps : 14500, \t Total Gen Loss : 32.71497344970703, \t Total Dis Loss : 0.00014909823948983103\n",
      "Steps : 14600, \t Total Gen Loss : 33.172786712646484, \t Total Dis Loss : 0.0005499057588167489\n",
      "Steps : 14700, \t Total Gen Loss : 29.951202392578125, \t Total Dis Loss : 0.000910719798412174\n",
      "Steps : 14800, \t Total Gen Loss : 32.51222610473633, \t Total Dis Loss : 0.0007472469587810338\n",
      "Steps : 14900, \t Total Gen Loss : 32.26791763305664, \t Total Dis Loss : 0.0006926454952917993\n",
      "Steps : 15000, \t Total Gen Loss : 31.369808197021484, \t Total Dis Loss : 0.00029258528957143426\n",
      "Steps : 15100, \t Total Gen Loss : 29.03571128845215, \t Total Dis Loss : 0.11641812324523926\n",
      "Steps : 15200, \t Total Gen Loss : 30.592578887939453, \t Total Dis Loss : 0.000755993474740535\n",
      "Steps : 15300, \t Total Gen Loss : 28.979095458984375, \t Total Dis Loss : 0.0023868773132562637\n",
      "Steps : 15400, \t Total Gen Loss : 29.03114891052246, \t Total Dis Loss : 0.001644705655053258\n",
      "Steps : 15500, \t Total Gen Loss : 32.86121368408203, \t Total Dis Loss : 0.0010929421987384558\n",
      "Steps : 15600, \t Total Gen Loss : 32.16816711425781, \t Total Dis Loss : 0.0006000575376674533\n",
      "Steps : 15700, \t Total Gen Loss : 31.18232536315918, \t Total Dis Loss : 0.00043386913603171706\n",
      "Steps : 15800, \t Total Gen Loss : 33.55410385131836, \t Total Dis Loss : 0.00025900619220919907\n",
      "Steps : 15900, \t Total Gen Loss : 28.83115005493164, \t Total Dis Loss : 0.009293526411056519\n",
      "Steps : 16000, \t Total Gen Loss : 31.04816246032715, \t Total Dis Loss : 0.00024570434470660985\n",
      "Steps : 16100, \t Total Gen Loss : 33.1507453918457, \t Total Dis Loss : 0.0007635528454557061\n",
      "Steps : 16200, \t Total Gen Loss : 29.787254333496094, \t Total Dis Loss : 0.001554535934701562\n",
      "Steps : 16300, \t Total Gen Loss : 33.26095962524414, \t Total Dis Loss : 0.00043262855615466833\n",
      "Steps : 16400, \t Total Gen Loss : 30.57482147216797, \t Total Dis Loss : 0.00036628369707614183\n",
      "Steps : 16500, \t Total Gen Loss : 31.271303176879883, \t Total Dis Loss : 0.005394679959863424\n",
      "Steps : 16600, \t Total Gen Loss : 33.818267822265625, \t Total Dis Loss : 0.00012874772073701024\n",
      "Steps : 16700, \t Total Gen Loss : 32.4224967956543, \t Total Dis Loss : 0.0007776680868119001\n",
      "Steps : 16800, \t Total Gen Loss : 31.332107543945312, \t Total Dis Loss : 0.0005389624275267124\n",
      "Steps : 16900, \t Total Gen Loss : 30.070539474487305, \t Total Dis Loss : 0.0009999098256230354\n",
      "Steps : 17000, \t Total Gen Loss : 31.731245040893555, \t Total Dis Loss : 0.0005800393410027027\n",
      "Steps : 17100, \t Total Gen Loss : 29.738536834716797, \t Total Dis Loss : 0.002637491561472416\n",
      "Steps : 17200, \t Total Gen Loss : 31.362895965576172, \t Total Dis Loss : 0.0007175349164754152\n",
      "Steps : 17300, \t Total Gen Loss : 30.557235717773438, \t Total Dis Loss : 0.0047837053425610065\n",
      "Steps : 17400, \t Total Gen Loss : 30.151744842529297, \t Total Dis Loss : 0.001623449963517487\n",
      "Steps : 17500, \t Total Gen Loss : 30.548704147338867, \t Total Dis Loss : 0.0006759564275853336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17600, \t Total Gen Loss : 30.50650405883789, \t Total Dis Loss : 0.0009151804260909557\n",
      "Steps : 17700, \t Total Gen Loss : 31.176546096801758, \t Total Dis Loss : 0.016383877024054527\n",
      "Steps : 17800, \t Total Gen Loss : 33.28278732299805, \t Total Dis Loss : 0.0002875876671168953\n",
      "Steps : 17900, \t Total Gen Loss : 33.561866760253906, \t Total Dis Loss : 0.0003887089842464775\n",
      "Steps : 18000, \t Total Gen Loss : 33.23715591430664, \t Total Dis Loss : 0.0003024609759449959\n",
      "Steps : 18100, \t Total Gen Loss : 30.796428680419922, \t Total Dis Loss : 0.0005613599205389619\n",
      "Steps : 18200, \t Total Gen Loss : 31.744915008544922, \t Total Dis Loss : 0.03600145876407623\n",
      "Steps : 18300, \t Total Gen Loss : 32.94775390625, \t Total Dis Loss : 0.0008923446293920279\n",
      "Steps : 18400, \t Total Gen Loss : 34.09077453613281, \t Total Dis Loss : 0.00036813082988373935\n",
      "Steps : 18500, \t Total Gen Loss : 30.129867553710938, \t Total Dis Loss : 0.0006896270788274705\n",
      "Steps : 18600, \t Total Gen Loss : 33.39690017700195, \t Total Dis Loss : 0.00032509583979845047\n",
      "Steps : 18700, \t Total Gen Loss : 31.92119598388672, \t Total Dis Loss : 0.0003874259127769619\n",
      "Steps : 18800, \t Total Gen Loss : 34.9405517578125, \t Total Dis Loss : 0.0002670128014869988\n",
      "Steps : 18900, \t Total Gen Loss : 32.58772277832031, \t Total Dis Loss : 0.00036910551716573536\n",
      "Steps : 19000, \t Total Gen Loss : 32.326053619384766, \t Total Dis Loss : 0.00021711613226216286\n",
      "Steps : 19100, \t Total Gen Loss : 35.18484115600586, \t Total Dis Loss : 0.00015115724818315357\n",
      "Steps : 19200, \t Total Gen Loss : 33.61424255371094, \t Total Dis Loss : 0.0003719382220879197\n",
      "Steps : 19300, \t Total Gen Loss : 32.27190399169922, \t Total Dis Loss : 0.0005081197014078498\n",
      "Steps : 19400, \t Total Gen Loss : 32.68899154663086, \t Total Dis Loss : 0.00048542002332396805\n",
      "Steps : 19500, \t Total Gen Loss : 33.20266342163086, \t Total Dis Loss : 0.00019939189951401204\n",
      "Steps : 19600, \t Total Gen Loss : 31.514070510864258, \t Total Dis Loss : 0.032935112714767456\n",
      "Steps : 19700, \t Total Gen Loss : 30.208072662353516, \t Total Dis Loss : 0.0025350088253617287\n",
      "Steps : 19800, \t Total Gen Loss : 32.0851936340332, \t Total Dis Loss : 0.000850297452416271\n",
      "Steps : 19900, \t Total Gen Loss : 33.588951110839844, \t Total Dis Loss : 0.000809717457741499\n",
      "Steps : 20000, \t Total Gen Loss : 30.531583786010742, \t Total Dis Loss : 0.000674945127684623\n",
      "Steps : 20100, \t Total Gen Loss : 34.776546478271484, \t Total Dis Loss : 0.00042819156078621745\n",
      "Steps : 20200, \t Total Gen Loss : 30.465057373046875, \t Total Dis Loss : 0.0013632754562422633\n",
      "Time for epoch 3 is 549.1543340682983 sec\n",
      "Steps : 20300, \t Total Gen Loss : 32.85759735107422, \t Total Dis Loss : 0.0013516065664589405\n",
      "Steps : 20400, \t Total Gen Loss : 33.10063171386719, \t Total Dis Loss : 0.0010819286108016968\n",
      "Steps : 20500, \t Total Gen Loss : 31.585102081298828, \t Total Dis Loss : 0.0007267719483934343\n",
      "Steps : 20600, \t Total Gen Loss : 33.137413024902344, \t Total Dis Loss : 0.0009456458501517773\n",
      "Steps : 20700, \t Total Gen Loss : 32.48201370239258, \t Total Dis Loss : 0.001269921544007957\n",
      "Steps : 20800, \t Total Gen Loss : 32.523502349853516, \t Total Dis Loss : 0.001167370704934001\n",
      "Steps : 20900, \t Total Gen Loss : 33.12345504760742, \t Total Dis Loss : 0.010023140348494053\n",
      "Steps : 21000, \t Total Gen Loss : 32.39334487915039, \t Total Dis Loss : 0.00024904176825657487\n",
      "Steps : 21100, \t Total Gen Loss : 31.52334213256836, \t Total Dis Loss : 0.000845740083605051\n",
      "Steps : 21200, \t Total Gen Loss : 33.46131896972656, \t Total Dis Loss : 0.0006025485345162451\n",
      "Steps : 21300, \t Total Gen Loss : 30.605899810791016, \t Total Dis Loss : 0.0008416125783696771\n",
      "Steps : 21400, \t Total Gen Loss : 32.77277374267578, \t Total Dis Loss : 0.000777954759541899\n",
      "Steps : 21500, \t Total Gen Loss : 31.825153350830078, \t Total Dis Loss : 0.0004967987770214677\n",
      "Steps : 21600, \t Total Gen Loss : 29.947038650512695, \t Total Dis Loss : 0.002600710606202483\n",
      "Steps : 21700, \t Total Gen Loss : 30.348886489868164, \t Total Dis Loss : 0.0016515622846782207\n",
      "Steps : 21800, \t Total Gen Loss : 29.455089569091797, \t Total Dis Loss : 0.0007648316095583141\n",
      "Steps : 21900, \t Total Gen Loss : 34.566619873046875, \t Total Dis Loss : 0.00027621525805443525\n",
      "Steps : 22000, \t Total Gen Loss : 30.08487319946289, \t Total Dis Loss : 0.0008182672900147736\n",
      "Steps : 22100, \t Total Gen Loss : 31.68770980834961, \t Total Dis Loss : 0.0009205957176163793\n",
      "Steps : 22200, \t Total Gen Loss : 33.39726257324219, \t Total Dis Loss : 0.0009771184995770454\n",
      "Steps : 22300, \t Total Gen Loss : 29.853885650634766, \t Total Dis Loss : 0.00041772276745177805\n",
      "Steps : 22400, \t Total Gen Loss : 32.26520919799805, \t Total Dis Loss : 0.00024004466831684113\n",
      "Steps : 22500, \t Total Gen Loss : 34.83378219604492, \t Total Dis Loss : 0.0001275261165574193\n",
      "Steps : 22600, \t Total Gen Loss : 31.082298278808594, \t Total Dis Loss : 0.0010580039815977216\n",
      "Steps : 22700, \t Total Gen Loss : 31.627965927124023, \t Total Dis Loss : 0.0005910740583203733\n",
      "Steps : 22800, \t Total Gen Loss : 33.21467971801758, \t Total Dis Loss : 0.0008112162468023598\n",
      "Steps : 22900, \t Total Gen Loss : 29.90741539001465, \t Total Dis Loss : 0.004665800370275974\n",
      "Steps : 23000, \t Total Gen Loss : 32.28675079345703, \t Total Dis Loss : 0.000292679644189775\n",
      "Steps : 23100, \t Total Gen Loss : 33.597389221191406, \t Total Dis Loss : 0.00027517665876075625\n",
      "Steps : 23200, \t Total Gen Loss : 33.209571838378906, \t Total Dis Loss : 0.00022007196093909442\n",
      "Steps : 23300, \t Total Gen Loss : 32.55960464477539, \t Total Dis Loss : 0.00043961938354186714\n",
      "Steps : 23400, \t Total Gen Loss : 30.376556396484375, \t Total Dis Loss : 0.00032620644196867943\n",
      "Steps : 23500, \t Total Gen Loss : 31.139314651489258, \t Total Dis Loss : 0.0005633083055727184\n",
      "Steps : 23600, \t Total Gen Loss : 29.860788345336914, \t Total Dis Loss : 0.0002203496842412278\n",
      "Steps : 23700, \t Total Gen Loss : 31.01290512084961, \t Total Dis Loss : 0.004783452022820711\n",
      "Steps : 23800, \t Total Gen Loss : 31.532358169555664, \t Total Dis Loss : 0.0006643066881224513\n",
      "Steps : 23900, \t Total Gen Loss : 32.003631591796875, \t Total Dis Loss : 0.0006629840354435146\n",
      "Steps : 24000, \t Total Gen Loss : 32.809513092041016, \t Total Dis Loss : 0.0004088379500899464\n",
      "Steps : 24100, \t Total Gen Loss : 32.56618118286133, \t Total Dis Loss : 0.000989521387964487\n",
      "Steps : 24200, \t Total Gen Loss : 30.320499420166016, \t Total Dis Loss : 0.0010635304497554898\n",
      "Steps : 24300, \t Total Gen Loss : 30.377286911010742, \t Total Dis Loss : 0.012230915948748589\n",
      "Steps : 24400, \t Total Gen Loss : 31.72047996520996, \t Total Dis Loss : 0.0007213596254587173\n",
      "Steps : 24500, \t Total Gen Loss : 31.29486846923828, \t Total Dis Loss : 0.00030412094201892614\n",
      "Steps : 24600, \t Total Gen Loss : 34.14458465576172, \t Total Dis Loss : 0.00032113626366481185\n",
      "Steps : 24700, \t Total Gen Loss : 31.84358787536621, \t Total Dis Loss : 0.0005066341836936772\n",
      "Steps : 24800, \t Total Gen Loss : 32.45841979980469, \t Total Dis Loss : 0.0012753872433677316\n",
      "Steps : 24900, \t Total Gen Loss : 35.60250473022461, \t Total Dis Loss : 0.0004457156755961478\n",
      "Steps : 25000, \t Total Gen Loss : 31.23670196533203, \t Total Dis Loss : 0.0006986094522289932\n",
      "Steps : 25100, \t Total Gen Loss : 32.550498962402344, \t Total Dis Loss : 0.00043953669955953956\n",
      "Steps : 25200, \t Total Gen Loss : 34.1046142578125, \t Total Dis Loss : 0.0022126450203359127\n",
      "Steps : 25300, \t Total Gen Loss : 33.42855453491211, \t Total Dis Loss : 0.00019618123769760132\n",
      "Steps : 25400, \t Total Gen Loss : 33.9432487487793, \t Total Dis Loss : 0.00019701497512869537\n",
      "Steps : 25500, \t Total Gen Loss : 29.917049407958984, \t Total Dis Loss : 0.0003504872147459537\n",
      "Steps : 25600, \t Total Gen Loss : 33.16565704345703, \t Total Dis Loss : 0.00019933254225179553\n",
      "Steps : 25700, \t Total Gen Loss : 30.11709213256836, \t Total Dis Loss : 0.0006520843598991632\n",
      "Steps : 25800, \t Total Gen Loss : 32.60092544555664, \t Total Dis Loss : 0.000607175228651613\n",
      "Steps : 25900, \t Total Gen Loss : 34.10499572753906, \t Total Dis Loss : 0.0002793814637698233\n",
      "Steps : 26000, \t Total Gen Loss : 32.200687408447266, \t Total Dis Loss : 0.0005220855819061399\n",
      "Steps : 26100, \t Total Gen Loss : 34.81273651123047, \t Total Dis Loss : 0.00017009387374855578\n",
      "Steps : 26200, \t Total Gen Loss : 32.76449966430664, \t Total Dis Loss : 0.0031745340675115585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26300, \t Total Gen Loss : 32.5034065246582, \t Total Dis Loss : 0.0007497017504647374\n",
      "Steps : 26400, \t Total Gen Loss : 30.448806762695312, \t Total Dis Loss : 0.0010241540148854256\n",
      "Steps : 26500, \t Total Gen Loss : 29.264389038085938, \t Total Dis Loss : 0.0008037680527195334\n",
      "Steps : 26600, \t Total Gen Loss : 30.541257858276367, \t Total Dis Loss : 0.0009344509453512728\n",
      "Steps : 26700, \t Total Gen Loss : 31.185640335083008, \t Total Dis Loss : 0.0010034403530880809\n",
      "Steps : 26800, \t Total Gen Loss : 33.95658874511719, \t Total Dis Loss : 0.0003410087083466351\n",
      "Steps : 26900, \t Total Gen Loss : 28.35906219482422, \t Total Dis Loss : 0.0005934683722443879\n",
      "Steps : 27000, \t Total Gen Loss : 32.86634063720703, \t Total Dis Loss : 0.0004330913652665913\n",
      "Time for epoch 4 is 543.0965387821198 sec\n",
      "Steps : 27100, \t Total Gen Loss : 31.007160186767578, \t Total Dis Loss : 0.0007003765786066651\n",
      "Steps : 27200, \t Total Gen Loss : 33.130027770996094, \t Total Dis Loss : 0.000553944380953908\n",
      "Steps : 27300, \t Total Gen Loss : 31.372417449951172, \t Total Dis Loss : 0.0006050810916349292\n",
      "Steps : 27400, \t Total Gen Loss : 30.320722579956055, \t Total Dis Loss : 0.001047209370881319\n",
      "Steps : 27500, \t Total Gen Loss : 33.287845611572266, \t Total Dis Loss : 0.0003106259973719716\n",
      "Steps : 27600, \t Total Gen Loss : 31.34339141845703, \t Total Dis Loss : 0.00031072660931386054\n",
      "Steps : 27700, \t Total Gen Loss : 33.20817184448242, \t Total Dis Loss : 0.0002597145503386855\n",
      "Steps : 27800, \t Total Gen Loss : 31.83449363708496, \t Total Dis Loss : 0.00024944497272372246\n",
      "Steps : 27900, \t Total Gen Loss : 33.92599105834961, \t Total Dis Loss : 0.0002683991624508053\n",
      "Steps : 28000, \t Total Gen Loss : 27.832168579101562, \t Total Dis Loss : 0.0034916324075311422\n",
      "Steps : 28100, \t Total Gen Loss : 32.28229904174805, \t Total Dis Loss : 0.0011072448687627912\n",
      "Steps : 28200, \t Total Gen Loss : 31.434356689453125, \t Total Dis Loss : 0.000763850228395313\n",
      "Steps : 28300, \t Total Gen Loss : 31.474233627319336, \t Total Dis Loss : 0.0010705027962103486\n",
      "Steps : 28400, \t Total Gen Loss : 32.638858795166016, \t Total Dis Loss : 0.00027761823730543256\n",
      "Steps : 28500, \t Total Gen Loss : 32.789581298828125, \t Total Dis Loss : 0.0001890369749162346\n",
      "Steps : 28600, \t Total Gen Loss : 31.33467674255371, \t Total Dis Loss : 0.0001453844306524843\n",
      "Steps : 28700, \t Total Gen Loss : 32.48047637939453, \t Total Dis Loss : 0.0003366706077940762\n",
      "Steps : 28800, \t Total Gen Loss : 32.475860595703125, \t Total Dis Loss : 0.00033377695945091546\n",
      "Steps : 28900, \t Total Gen Loss : 32.31462097167969, \t Total Dis Loss : 0.00015025278844404966\n",
      "Steps : 29000, \t Total Gen Loss : 30.89925193786621, \t Total Dis Loss : 0.00012533090193755925\n",
      "Steps : 29100, \t Total Gen Loss : 29.974334716796875, \t Total Dis Loss : 0.00021148056839592755\n",
      "Steps : 29200, \t Total Gen Loss : 33.495765686035156, \t Total Dis Loss : 0.00024573851260356605\n",
      "Steps : 29300, \t Total Gen Loss : 31.215253829956055, \t Total Dis Loss : 0.0002106634055962786\n",
      "Steps : 29400, \t Total Gen Loss : 32.91990280151367, \t Total Dis Loss : 0.000304793706163764\n",
      "Steps : 29500, \t Total Gen Loss : 32.169620513916016, \t Total Dis Loss : 0.000198412177269347\n",
      "Steps : 29600, \t Total Gen Loss : 29.6954288482666, \t Total Dis Loss : 0.00022437107691075653\n",
      "Steps : 29700, \t Total Gen Loss : 29.914485931396484, \t Total Dis Loss : 0.0002567142655607313\n",
      "Steps : 29800, \t Total Gen Loss : 30.8555965423584, \t Total Dis Loss : 0.0002665257197804749\n",
      "Steps : 29900, \t Total Gen Loss : 28.330629348754883, \t Total Dis Loss : 0.002239345805719495\n",
      "Steps : 30000, \t Total Gen Loss : 31.372894287109375, \t Total Dis Loss : 0.0006110910326242447\n",
      "Steps : 30100, \t Total Gen Loss : 27.260848999023438, \t Total Dis Loss : 0.00838108453899622\n",
      "Steps : 30200, \t Total Gen Loss : 30.927738189697266, \t Total Dis Loss : 0.0019639083184301853\n",
      "Steps : 30300, \t Total Gen Loss : 35.25259780883789, \t Total Dis Loss : 0.005423054099082947\n",
      "Steps : 30400, \t Total Gen Loss : 34.34699249267578, \t Total Dis Loss : 0.00023534649517387152\n",
      "Steps : 30500, \t Total Gen Loss : 35.355186462402344, \t Total Dis Loss : 0.0005858984077349305\n",
      "Steps : 30600, \t Total Gen Loss : 32.634891510009766, \t Total Dis Loss : 0.00040798421832732856\n",
      "Steps : 30700, \t Total Gen Loss : 30.243806838989258, \t Total Dis Loss : 0.0003924877382814884\n",
      "Steps : 30800, \t Total Gen Loss : 30.627912521362305, \t Total Dis Loss : 0.00022652145707979798\n",
      "Steps : 30900, \t Total Gen Loss : 33.02594757080078, \t Total Dis Loss : 0.0006502901669591665\n",
      "Steps : 31000, \t Total Gen Loss : 31.95652961730957, \t Total Dis Loss : 0.00026847884873859584\n",
      "Steps : 31100, \t Total Gen Loss : 28.565711975097656, \t Total Dis Loss : 0.0008755273302085698\n",
      "Steps : 31200, \t Total Gen Loss : 33.28089904785156, \t Total Dis Loss : 0.0004612391348928213\n",
      "Steps : 31300, \t Total Gen Loss : 31.455961227416992, \t Total Dis Loss : 0.00047022683429531753\n",
      "Steps : 31400, \t Total Gen Loss : 32.887901306152344, \t Total Dis Loss : 0.0002744635858107358\n",
      "Steps : 31500, \t Total Gen Loss : 31.871238708496094, \t Total Dis Loss : 0.0003418915730435401\n",
      "Steps : 31600, \t Total Gen Loss : 30.01360321044922, \t Total Dis Loss : 0.000749894417822361\n",
      "Steps : 31700, \t Total Gen Loss : 31.229642868041992, \t Total Dis Loss : 0.00031597522320225835\n",
      "Steps : 31800, \t Total Gen Loss : 29.962268829345703, \t Total Dis Loss : 0.0010847888188436627\n",
      "Steps : 31900, \t Total Gen Loss : 30.661256790161133, \t Total Dis Loss : 0.0007370305829681456\n",
      "Steps : 32000, \t Total Gen Loss : 33.58205795288086, \t Total Dis Loss : 0.00033674840233288705\n",
      "Steps : 32100, \t Total Gen Loss : 31.99985122680664, \t Total Dis Loss : 0.0006085811764933169\n",
      "Steps : 32200, \t Total Gen Loss : 32.64457321166992, \t Total Dis Loss : 0.0015754750929772854\n",
      "Steps : 32300, \t Total Gen Loss : 32.4179801940918, \t Total Dis Loss : 0.0005055496003478765\n",
      "Steps : 32400, \t Total Gen Loss : 32.74266815185547, \t Total Dis Loss : 0.0018881188007071614\n",
      "Steps : 32500, \t Total Gen Loss : 32.73826599121094, \t Total Dis Loss : 0.0005428250296972692\n",
      "Steps : 32600, \t Total Gen Loss : 29.149951934814453, \t Total Dis Loss : 0.0007637544185854495\n",
      "Steps : 32700, \t Total Gen Loss : 28.476709365844727, \t Total Dis Loss : 0.0009507664362899959\n",
      "Steps : 32800, \t Total Gen Loss : 31.01429557800293, \t Total Dis Loss : 0.0010505270911380649\n",
      "Steps : 32900, \t Total Gen Loss : 31.41292953491211, \t Total Dis Loss : 0.0009211802971549332\n",
      "Steps : 33000, \t Total Gen Loss : 33.54587936401367, \t Total Dis Loss : 0.0002660542377270758\n",
      "Steps : 33100, \t Total Gen Loss : 30.645824432373047, \t Total Dis Loss : 0.0003894937690347433\n",
      "Steps : 33200, \t Total Gen Loss : 31.010051727294922, \t Total Dis Loss : 0.0015168357640504837\n",
      "Steps : 33300, \t Total Gen Loss : 30.033405303955078, \t Total Dis Loss : 0.000411917018936947\n",
      "Steps : 33400, \t Total Gen Loss : 31.333450317382812, \t Total Dis Loss : 0.0014925961149856448\n",
      "Steps : 33500, \t Total Gen Loss : 32.203487396240234, \t Total Dis Loss : 0.0006835052627138793\n",
      "Steps : 33600, \t Total Gen Loss : 30.591230392456055, \t Total Dis Loss : 0.005234223324805498\n",
      "Steps : 33700, \t Total Gen Loss : 29.97802734375, \t Total Dis Loss : 0.0009500908199697733\n",
      "Time for epoch 5 is 542.4573497772217 sec\n",
      "Steps : 33800, \t Total Gen Loss : 30.589426040649414, \t Total Dis Loss : 0.0003751660115085542\n",
      "Steps : 33900, \t Total Gen Loss : 32.733551025390625, \t Total Dis Loss : 0.0003839007404167205\n",
      "Steps : 34000, \t Total Gen Loss : 32.09925842285156, \t Total Dis Loss : 0.0006383640575222671\n",
      "Steps : 34100, \t Total Gen Loss : 30.5210018157959, \t Total Dis Loss : 0.001039593480527401\n",
      "Steps : 34200, \t Total Gen Loss : 31.710643768310547, \t Total Dis Loss : 0.00028099078917875886\n",
      "Steps : 34300, \t Total Gen Loss : 34.38199996948242, \t Total Dis Loss : 0.00023532711202278733\n",
      "Steps : 34400, \t Total Gen Loss : 32.605628967285156, \t Total Dis Loss : 0.0002788919082377106\n",
      "Steps : 34500, \t Total Gen Loss : 33.506168365478516, \t Total Dis Loss : 0.00019998537027277052\n",
      "Steps : 34600, \t Total Gen Loss : 32.6434326171875, \t Total Dis Loss : 0.00022325615282170475\n",
      "Steps : 34700, \t Total Gen Loss : 32.83650207519531, \t Total Dis Loss : 0.0003125760704278946\n",
      "Steps : 34800, \t Total Gen Loss : 32.89381408691406, \t Total Dis Loss : 0.00022859949967823923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 34900, \t Total Gen Loss : 31.910289764404297, \t Total Dis Loss : 0.0018472003284841776\n",
      "Steps : 35000, \t Total Gen Loss : 30.68623924255371, \t Total Dis Loss : 0.0015382550191134214\n",
      "Steps : 35100, \t Total Gen Loss : 33.238006591796875, \t Total Dis Loss : 0.0003223716630600393\n",
      "Steps : 35200, \t Total Gen Loss : 30.55084228515625, \t Total Dis Loss : 0.005204136483371258\n",
      "Steps : 35300, \t Total Gen Loss : 30.296022415161133, \t Total Dis Loss : 0.0014926684089004993\n",
      "Steps : 35400, \t Total Gen Loss : 30.468172073364258, \t Total Dis Loss : 0.0007345216581597924\n",
      "Steps : 35500, \t Total Gen Loss : 30.907997131347656, \t Total Dis Loss : 0.0008722483762539923\n",
      "Steps : 35600, \t Total Gen Loss : 30.526025772094727, \t Total Dis Loss : 0.0003281162353232503\n",
      "Steps : 35700, \t Total Gen Loss : 32.968963623046875, \t Total Dis Loss : 0.0001617273810552433\n",
      "Steps : 35800, \t Total Gen Loss : 32.32097244262695, \t Total Dis Loss : 0.0002358811761951074\n",
      "Steps : 35900, \t Total Gen Loss : 34.26161193847656, \t Total Dis Loss : 0.00018977672152686864\n",
      "Steps : 36000, \t Total Gen Loss : 31.600418090820312, \t Total Dis Loss : 0.0001530320441816002\n",
      "Steps : 36100, \t Total Gen Loss : 31.385700225830078, \t Total Dis Loss : 0.00027976022101938725\n",
      "Steps : 36200, \t Total Gen Loss : 29.78268814086914, \t Total Dis Loss : 0.0002549978089518845\n",
      "Steps : 36300, \t Total Gen Loss : 32.61479568481445, \t Total Dis Loss : 0.00018244789680466056\n",
      "Steps : 36400, \t Total Gen Loss : 34.05125427246094, \t Total Dis Loss : 0.0001445971429347992\n",
      "Steps : 36500, \t Total Gen Loss : 33.61537551879883, \t Total Dis Loss : 0.00014508784806821495\n",
      "Steps : 36600, \t Total Gen Loss : 31.14471435546875, \t Total Dis Loss : 0.00037134462036192417\n",
      "Steps : 36700, \t Total Gen Loss : 34.02799606323242, \t Total Dis Loss : 0.0001606431760592386\n",
      "Steps : 36800, \t Total Gen Loss : 34.172603607177734, \t Total Dis Loss : 8.108405745588243e-05\n",
      "Steps : 36900, \t Total Gen Loss : 32.791725158691406, \t Total Dis Loss : 0.0004964435938745737\n",
      "Steps : 37000, \t Total Gen Loss : 30.904664993286133, \t Total Dis Loss : 0.0002533221850171685\n",
      "Steps : 37100, \t Total Gen Loss : 30.20216941833496, \t Total Dis Loss : 0.0003682004753500223\n",
      "Steps : 37200, \t Total Gen Loss : 33.143489837646484, \t Total Dis Loss : 0.00263193529099226\n",
      "Steps : 37300, \t Total Gen Loss : 32.617618560791016, \t Total Dis Loss : 0.00016976738697849214\n",
      "Steps : 37400, \t Total Gen Loss : 34.48526382446289, \t Total Dis Loss : 0.0002278073225170374\n",
      "Steps : 37500, \t Total Gen Loss : 32.89427947998047, \t Total Dis Loss : 0.00035006270627491176\n",
      "Steps : 37600, \t Total Gen Loss : 30.437454223632812, \t Total Dis Loss : 0.00015453220112249255\n",
      "Steps : 37700, \t Total Gen Loss : 32.2711067199707, \t Total Dis Loss : 9.528095688438043e-05\n",
      "Steps : 37800, \t Total Gen Loss : 29.255098342895508, \t Total Dis Loss : 0.006957028992474079\n",
      "Steps : 37900, \t Total Gen Loss : 33.1046028137207, \t Total Dis Loss : 0.00020964638679288328\n",
      "Steps : 38000, \t Total Gen Loss : 31.13361167907715, \t Total Dis Loss : 0.0005595511174760759\n",
      "Steps : 38100, \t Total Gen Loss : 30.640331268310547, \t Total Dis Loss : 0.0003747935115825385\n",
      "Steps : 38200, \t Total Gen Loss : 31.409578323364258, \t Total Dis Loss : 0.000979722011834383\n",
      "Steps : 38300, \t Total Gen Loss : 29.873703002929688, \t Total Dis Loss : 0.0003991147968918085\n",
      "Steps : 38400, \t Total Gen Loss : 32.63378143310547, \t Total Dis Loss : 0.00019844068447127938\n",
      "Steps : 38500, \t Total Gen Loss : 30.081497192382812, \t Total Dis Loss : 0.0012629161356016994\n",
      "Steps : 38600, \t Total Gen Loss : 33.0987548828125, \t Total Dis Loss : 0.0010659274412319064\n",
      "Steps : 38700, \t Total Gen Loss : 31.415979385375977, \t Total Dis Loss : 0.0007162675028666854\n",
      "Steps : 38800, \t Total Gen Loss : 32.699676513671875, \t Total Dis Loss : 0.00011402120435377583\n",
      "Steps : 38900, \t Total Gen Loss : 34.09565734863281, \t Total Dis Loss : 0.002807073527947068\n",
      "Steps : 39000, \t Total Gen Loss : 30.906993865966797, \t Total Dis Loss : 0.00217854930087924\n",
      "Steps : 39100, \t Total Gen Loss : 36.48881530761719, \t Total Dis Loss : 0.0015065516345202923\n",
      "Steps : 39200, \t Total Gen Loss : 31.45577621459961, \t Total Dis Loss : 0.0003245239204261452\n",
      "Steps : 39300, \t Total Gen Loss : 30.64068603515625, \t Total Dis Loss : 0.00034529005642980337\n",
      "Steps : 39400, \t Total Gen Loss : 31.61898422241211, \t Total Dis Loss : 0.000363971950719133\n",
      "Steps : 39500, \t Total Gen Loss : 28.29109001159668, \t Total Dis Loss : 0.010922873392701149\n",
      "Steps : 39600, \t Total Gen Loss : 34.32079315185547, \t Total Dis Loss : 0.00018394971266388893\n",
      "Steps : 39700, \t Total Gen Loss : 34.259674072265625, \t Total Dis Loss : 0.00031921028858050704\n",
      "Steps : 39800, \t Total Gen Loss : 32.98207473754883, \t Total Dis Loss : 0.0002738162875175476\n",
      "Steps : 39900, \t Total Gen Loss : 33.87950134277344, \t Total Dis Loss : 0.00019015438738279045\n",
      "Steps : 40000, \t Total Gen Loss : 33.95203399658203, \t Total Dis Loss : 0.0001852525892900303\n",
      "Steps : 40100, \t Total Gen Loss : 32.16356658935547, \t Total Dis Loss : 0.00029378634644672275\n",
      "Steps : 40200, \t Total Gen Loss : 35.159088134765625, \t Total Dis Loss : 0.00014391809236258268\n",
      "Steps : 40300, \t Total Gen Loss : 32.64009475708008, \t Total Dis Loss : 8.068344322964549e-05\n",
      "Steps : 40400, \t Total Gen Loss : 32.82781219482422, \t Total Dis Loss : 0.00041072512976825237\n",
      "Steps : 40500, \t Total Gen Loss : 33.365570068359375, \t Total Dis Loss : 0.00017740843759384006\n",
      "Time for epoch 6 is 544.1413333415985 sec\n",
      "Steps : 40600, \t Total Gen Loss : 34.6038818359375, \t Total Dis Loss : 0.0002195581910200417\n",
      "Steps : 40700, \t Total Gen Loss : 32.94562911987305, \t Total Dis Loss : 0.0007980826776474714\n",
      "Steps : 40800, \t Total Gen Loss : 33.604366302490234, \t Total Dis Loss : 0.0005207262001931667\n",
      "Steps : 40900, \t Total Gen Loss : 31.104785919189453, \t Total Dis Loss : 0.0006197583861649036\n",
      "Steps : 41000, \t Total Gen Loss : 31.5373592376709, \t Total Dis Loss : 0.000342142564477399\n",
      "Steps : 41100, \t Total Gen Loss : 32.06891632080078, \t Total Dis Loss : 0.0001826680963858962\n",
      "Steps : 41200, \t Total Gen Loss : 31.87896728515625, \t Total Dis Loss : 0.00019093876471742988\n",
      "Steps : 41300, \t Total Gen Loss : 30.247486114501953, \t Total Dis Loss : 0.000840991095174104\n",
      "Steps : 41400, \t Total Gen Loss : 31.4014949798584, \t Total Dis Loss : 0.0003985350485891104\n",
      "Steps : 41500, \t Total Gen Loss : 31.74862289428711, \t Total Dis Loss : 0.0007368531078100204\n",
      "Steps : 41600, \t Total Gen Loss : 31.071151733398438, \t Total Dis Loss : 0.1462949961423874\n",
      "Steps : 41700, \t Total Gen Loss : 32.154991149902344, \t Total Dis Loss : 0.0003064035845454782\n",
      "Steps : 41800, \t Total Gen Loss : 33.416561126708984, \t Total Dis Loss : 0.00029456138145178556\n",
      "Steps : 41900, \t Total Gen Loss : 32.6457633972168, \t Total Dis Loss : 0.000603604014031589\n",
      "Steps : 42000, \t Total Gen Loss : 32.03382110595703, \t Total Dis Loss : 0.0005700532346963882\n",
      "Steps : 42100, \t Total Gen Loss : 31.156185150146484, \t Total Dis Loss : 0.0005340248462744057\n",
      "Steps : 42200, \t Total Gen Loss : 31.941085815429688, \t Total Dis Loss : 0.00027025441522710025\n",
      "Steps : 42300, \t Total Gen Loss : 32.964298248291016, \t Total Dis Loss : 0.00031943328212946653\n",
      "Steps : 42400, \t Total Gen Loss : 31.33013343811035, \t Total Dis Loss : 0.0002754312881734222\n",
      "Steps : 42500, \t Total Gen Loss : 32.85209274291992, \t Total Dis Loss : 0.00018642320355866104\n",
      "Steps : 42600, \t Total Gen Loss : 29.345256805419922, \t Total Dis Loss : 0.0005312107969075441\n",
      "Steps : 42700, \t Total Gen Loss : 32.574188232421875, \t Total Dis Loss : 0.0005200362065806985\n",
      "Steps : 42800, \t Total Gen Loss : 33.02525329589844, \t Total Dis Loss : 0.001209188369102776\n",
      "Steps : 42900, \t Total Gen Loss : 31.17469024658203, \t Total Dis Loss : 0.0003623793600127101\n",
      "Steps : 43000, \t Total Gen Loss : 33.77309036254883, \t Total Dis Loss : 0.0003194609598722309\n",
      "Steps : 43100, \t Total Gen Loss : 36.75109100341797, \t Total Dis Loss : 9.149103425443172e-05\n",
      "Steps : 43200, \t Total Gen Loss : 35.030799865722656, \t Total Dis Loss : 6.29158312221989e-05\n",
      "Steps : 43300, \t Total Gen Loss : 34.252445220947266, \t Total Dis Loss : 0.00038548343582078815\n",
      "Steps : 43400, \t Total Gen Loss : 31.920330047607422, \t Total Dis Loss : 0.0002843370020855218\n",
      "Steps : 43500, \t Total Gen Loss : 32.97507858276367, \t Total Dis Loss : 0.000291073985863477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 43600, \t Total Gen Loss : 31.020402908325195, \t Total Dis Loss : 0.009797366335988045\n",
      "Steps : 43700, \t Total Gen Loss : 31.499317169189453, \t Total Dis Loss : 0.0004553029721137136\n",
      "Steps : 43800, \t Total Gen Loss : 31.67433738708496, \t Total Dis Loss : 0.01623028889298439\n",
      "Steps : 43900, \t Total Gen Loss : 34.80731201171875, \t Total Dis Loss : 0.0005827970453538001\n",
      "Steps : 44000, \t Total Gen Loss : 32.52281188964844, \t Total Dis Loss : 0.15332633256912231\n",
      "Steps : 44100, \t Total Gen Loss : 31.227157592773438, \t Total Dis Loss : 0.000663430371787399\n",
      "Steps : 44200, \t Total Gen Loss : 32.540489196777344, \t Total Dis Loss : 0.00035636546090245247\n",
      "Steps : 44300, \t Total Gen Loss : 34.251708984375, \t Total Dis Loss : 0.00019279606931377202\n",
      "Steps : 44400, \t Total Gen Loss : 30.07342529296875, \t Total Dis Loss : 0.002920759841799736\n",
      "Steps : 44500, \t Total Gen Loss : 31.965923309326172, \t Total Dis Loss : 0.0005032876506447792\n",
      "Steps : 44600, \t Total Gen Loss : 33.17439270019531, \t Total Dis Loss : 0.0003104961942881346\n",
      "Steps : 44700, \t Total Gen Loss : 32.90750503540039, \t Total Dis Loss : 0.00017519405810162425\n",
      "Steps : 44800, \t Total Gen Loss : 30.101848602294922, \t Total Dis Loss : 0.00020304603094700724\n",
      "Steps : 44900, \t Total Gen Loss : 31.45539665222168, \t Total Dis Loss : 0.0003082278126385063\n",
      "Steps : 45000, \t Total Gen Loss : 30.829532623291016, \t Total Dis Loss : 0.0008901115506887436\n",
      "Steps : 45100, \t Total Gen Loss : 32.354957580566406, \t Total Dis Loss : 0.0007659092079848051\n",
      "Steps : 45200, \t Total Gen Loss : 32.14917755126953, \t Total Dis Loss : 0.0003945658972952515\n",
      "Steps : 45300, \t Total Gen Loss : 31.046798706054688, \t Total Dis Loss : 0.0003833899972960353\n",
      "Steps : 45400, \t Total Gen Loss : 33.40826416015625, \t Total Dis Loss : 0.0004099231446161866\n",
      "Steps : 45500, \t Total Gen Loss : 32.05297088623047, \t Total Dis Loss : 0.00019870276446454227\n",
      "Steps : 45600, \t Total Gen Loss : 31.7255802154541, \t Total Dis Loss : 0.0009021179284900427\n",
      "Steps : 45700, \t Total Gen Loss : 31.63191032409668, \t Total Dis Loss : 0.00036106572952121496\n",
      "Steps : 45800, \t Total Gen Loss : 32.840858459472656, \t Total Dis Loss : 0.00018043852469418198\n",
      "Steps : 45900, \t Total Gen Loss : 32.231544494628906, \t Total Dis Loss : 0.0008983073057606816\n",
      "Steps : 46000, \t Total Gen Loss : 30.453786849975586, \t Total Dis Loss : 0.0021345713175833225\n",
      "Steps : 46100, \t Total Gen Loss : 30.97722625732422, \t Total Dis Loss : 0.0004160990356467664\n",
      "Steps : 46200, \t Total Gen Loss : 31.569198608398438, \t Total Dis Loss : 0.00033578459988348186\n",
      "Steps : 46300, \t Total Gen Loss : 31.561525344848633, \t Total Dis Loss : 0.00039195860153995454\n",
      "Steps : 46400, \t Total Gen Loss : 37.6921501159668, \t Total Dis Loss : 0.0003944607451558113\n",
      "Steps : 46500, \t Total Gen Loss : 34.23692321777344, \t Total Dis Loss : 0.00018044220632873476\n",
      "Steps : 46600, \t Total Gen Loss : 33.50861358642578, \t Total Dis Loss : 0.000533715938217938\n",
      "Steps : 46700, \t Total Gen Loss : 31.941219329833984, \t Total Dis Loss : 0.0013297374825924635\n",
      "Steps : 46800, \t Total Gen Loss : 32.1732063293457, \t Total Dis Loss : 0.00046082938206382096\n",
      "Steps : 46900, \t Total Gen Loss : 32.95746612548828, \t Total Dis Loss : 0.00037570594577118754\n",
      "Steps : 47000, \t Total Gen Loss : 32.553131103515625, \t Total Dis Loss : 0.00018372373597230762\n",
      "Steps : 47100, \t Total Gen Loss : 33.736907958984375, \t Total Dis Loss : 0.0004832424165215343\n",
      "Steps : 47200, \t Total Gen Loss : 31.83509063720703, \t Total Dis Loss : 0.00045695743756368756\n",
      "Time for epoch 7 is 542.7277641296387 sec\n",
      "Steps : 47300, \t Total Gen Loss : 33.725093841552734, \t Total Dis Loss : 0.00036313600139692426\n",
      "Steps : 47400, \t Total Gen Loss : 31.43332862854004, \t Total Dis Loss : 0.00025177045608870685\n",
      "Steps : 47500, \t Total Gen Loss : 32.04625701904297, \t Total Dis Loss : 0.000285617628833279\n",
      "Steps : 47600, \t Total Gen Loss : 31.296749114990234, \t Total Dis Loss : 0.00027685315581038594\n",
      "Steps : 47700, \t Total Gen Loss : 33.21160125732422, \t Total Dis Loss : 0.0003366950841154903\n",
      "Steps : 47800, \t Total Gen Loss : 31.49294662475586, \t Total Dis Loss : 0.0005533458897843957\n",
      "Steps : 47900, \t Total Gen Loss : 33.0751838684082, \t Total Dis Loss : 0.0007554355543106794\n",
      "Steps : 48000, \t Total Gen Loss : 30.274423599243164, \t Total Dis Loss : 0.0003970643738284707\n",
      "Steps : 48100, \t Total Gen Loss : 30.399497985839844, \t Total Dis Loss : 0.0005332633154466748\n",
      "Steps : 48200, \t Total Gen Loss : 32.5127067565918, \t Total Dis Loss : 0.00317865121178329\n",
      "Steps : 48300, \t Total Gen Loss : 32.07893753051758, \t Total Dis Loss : 0.0003247823042329401\n",
      "Steps : 48400, \t Total Gen Loss : 32.295040130615234, \t Total Dis Loss : 0.00020284359925426543\n",
      "Steps : 48500, \t Total Gen Loss : 33.377601623535156, \t Total Dis Loss : 0.00012156637967564166\n",
      "Steps : 48600, \t Total Gen Loss : 33.43272399902344, \t Total Dis Loss : 0.00018922223534900695\n",
      "Steps : 48700, \t Total Gen Loss : 33.113609313964844, \t Total Dis Loss : 0.00010910742275882512\n",
      "Steps : 48800, \t Total Gen Loss : 30.835874557495117, \t Total Dis Loss : 0.00013210180622991174\n",
      "Steps : 48900, \t Total Gen Loss : 35.792198181152344, \t Total Dis Loss : 0.00011026777792721987\n",
      "Steps : 49000, \t Total Gen Loss : 37.570552825927734, \t Total Dis Loss : 0.0011953883804380894\n",
      "Steps : 49100, \t Total Gen Loss : 34.17398452758789, \t Total Dis Loss : 0.0008987366454675794\n",
      "Steps : 49200, \t Total Gen Loss : 36.41701126098633, \t Total Dis Loss : 6.627749098697677e-05\n",
      "Steps : 49300, \t Total Gen Loss : 31.285558700561523, \t Total Dis Loss : 0.00031515915179625154\n",
      "Steps : 49400, \t Total Gen Loss : 32.198368072509766, \t Total Dis Loss : 0.0005092804203741252\n",
      "Steps : 49500, \t Total Gen Loss : 32.2850456237793, \t Total Dis Loss : 0.00038047361886128783\n",
      "Steps : 49600, \t Total Gen Loss : 30.388364791870117, \t Total Dis Loss : 0.0011058596428483725\n",
      "Steps : 49700, \t Total Gen Loss : 33.765098571777344, \t Total Dis Loss : 0.0006454440881498158\n",
      "Steps : 49800, \t Total Gen Loss : 28.21338653564453, \t Total Dis Loss : 0.0030501915607601404\n",
      "Steps : 49900, \t Total Gen Loss : 29.531396865844727, \t Total Dis Loss : 0.002409935463219881\n",
      "Steps : 50000, \t Total Gen Loss : 32.53082275390625, \t Total Dis Loss : 0.0006376871606335044\n",
      "Steps : 50100, \t Total Gen Loss : 33.530818939208984, \t Total Dis Loss : 0.0001505814288975671\n",
      "Steps : 50200, \t Total Gen Loss : 33.69220733642578, \t Total Dis Loss : 0.0001509007706772536\n",
      "Steps : 50300, \t Total Gen Loss : 33.7310676574707, \t Total Dis Loss : 0.00020483692060224712\n",
      "Steps : 50400, \t Total Gen Loss : 32.21636962890625, \t Total Dis Loss : 0.0002105560852214694\n",
      "Steps : 50500, \t Total Gen Loss : 32.144447326660156, \t Total Dis Loss : 0.0002149512292817235\n",
      "Steps : 50600, \t Total Gen Loss : 31.538484573364258, \t Total Dis Loss : 0.0004592719778884202\n",
      "Steps : 50700, \t Total Gen Loss : 32.87968063354492, \t Total Dis Loss : 0.0002585032780189067\n",
      "Steps : 50800, \t Total Gen Loss : 32.75804901123047, \t Total Dis Loss : 0.00025216248468495905\n",
      "Steps : 50900, \t Total Gen Loss : 30.845664978027344, \t Total Dis Loss : 0.1327042579650879\n",
      "Steps : 51000, \t Total Gen Loss : 31.93521499633789, \t Total Dis Loss : 0.0006670778384432197\n",
      "Steps : 51100, \t Total Gen Loss : 33.829200744628906, \t Total Dis Loss : 0.00012395081284921616\n",
      "Steps : 51200, \t Total Gen Loss : 32.46321105957031, \t Total Dis Loss : 0.06378968805074692\n",
      "Steps : 51300, \t Total Gen Loss : 33.88190460205078, \t Total Dis Loss : 0.0001981633249670267\n",
      "Steps : 51400, \t Total Gen Loss : 32.69175338745117, \t Total Dis Loss : 0.0005881401593796909\n",
      "Steps : 51500, \t Total Gen Loss : 34.31086349487305, \t Total Dis Loss : 0.0001721876469673589\n",
      "Steps : 51600, \t Total Gen Loss : 32.3120231628418, \t Total Dis Loss : 0.04522987827658653\n",
      "Steps : 51700, \t Total Gen Loss : 34.49980545043945, \t Total Dis Loss : 0.0012279662769287825\n",
      "Steps : 51800, \t Total Gen Loss : 36.8187141418457, \t Total Dis Loss : 7.601175457239151e-05\n",
      "Steps : 51900, \t Total Gen Loss : 35.88958740234375, \t Total Dis Loss : 9.196801693178713e-05\n",
      "Steps : 52000, \t Total Gen Loss : 36.07075881958008, \t Total Dis Loss : 9.490362572250888e-05\n",
      "Steps : 52100, \t Total Gen Loss : 35.3261833190918, \t Total Dis Loss : 0.0005005383281968534\n",
      "Steps : 52200, \t Total Gen Loss : 31.34634017944336, \t Total Dis Loss : 0.0007279949495568871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 52300, \t Total Gen Loss : 30.394563674926758, \t Total Dis Loss : 0.0037002312019467354\n",
      "Steps : 52400, \t Total Gen Loss : 31.427053451538086, \t Total Dis Loss : 0.0014927656156942248\n",
      "Steps : 52500, \t Total Gen Loss : 33.206722259521484, \t Total Dis Loss : 8.540348790120333e-05\n",
      "Steps : 52600, \t Total Gen Loss : 35.15335464477539, \t Total Dis Loss : 0.00029560650000348687\n",
      "Steps : 52700, \t Total Gen Loss : 31.32646369934082, \t Total Dis Loss : 0.001049028243869543\n",
      "Steps : 52800, \t Total Gen Loss : 33.19643783569336, \t Total Dis Loss : 5.884052370674908e-05\n",
      "Steps : 52900, \t Total Gen Loss : 29.581222534179688, \t Total Dis Loss : 0.0015012247022241354\n",
      "Steps : 53000, \t Total Gen Loss : 33.43572235107422, \t Total Dis Loss : 0.0004150604654569179\n",
      "Steps : 53100, \t Total Gen Loss : 30.62825584411621, \t Total Dis Loss : 0.010195535607635975\n",
      "Steps : 53200, \t Total Gen Loss : 33.98604965209961, \t Total Dis Loss : 0.00046682701213285327\n",
      "Steps : 53300, \t Total Gen Loss : 33.81538772583008, \t Total Dis Loss : 0.00027190466062165797\n",
      "Steps : 53400, \t Total Gen Loss : 32.909358978271484, \t Total Dis Loss : 0.00010493199806660414\n",
      "Steps : 53500, \t Total Gen Loss : 33.81047058105469, \t Total Dis Loss : 0.00029473708127625287\n",
      "Steps : 53600, \t Total Gen Loss : 34.824642181396484, \t Total Dis Loss : 0.00019508309196680784\n",
      "Steps : 53700, \t Total Gen Loss : 33.0915412902832, \t Total Dis Loss : 0.0002363063395023346\n",
      "Steps : 53800, \t Total Gen Loss : 30.894121170043945, \t Total Dis Loss : 0.00023032673925627023\n",
      "Steps : 53900, \t Total Gen Loss : 32.3615608215332, \t Total Dis Loss : 0.0001228607725352049\n",
      "Steps : 54000, \t Total Gen Loss : 32.92133331298828, \t Total Dis Loss : 0.0002461474505253136\n",
      "Time for epoch 8 is 542.0610320568085 sec\n",
      "Steps : 54100, \t Total Gen Loss : 32.43446350097656, \t Total Dis Loss : 0.0002040968684013933\n",
      "Steps : 54200, \t Total Gen Loss : 33.36760711669922, \t Total Dis Loss : 0.001015426591038704\n",
      "Steps : 54300, \t Total Gen Loss : 34.35152053833008, \t Total Dis Loss : 0.00020726292859762907\n",
      "Steps : 54400, \t Total Gen Loss : 32.771018981933594, \t Total Dis Loss : 0.00017530520563013852\n",
      "Steps : 54500, \t Total Gen Loss : 35.32756805419922, \t Total Dis Loss : 0.00010056614701170474\n",
      "Steps : 54600, \t Total Gen Loss : 32.61000061035156, \t Total Dis Loss : 0.00030559379956685007\n",
      "Steps : 54700, \t Total Gen Loss : 33.55876159667969, \t Total Dis Loss : 0.000539169879630208\n",
      "Steps : 54800, \t Total Gen Loss : 30.978113174438477, \t Total Dis Loss : 0.0002560769789852202\n",
      "Steps : 54900, \t Total Gen Loss : 33.24177551269531, \t Total Dis Loss : 0.00025063977227546275\n",
      "Steps : 55000, \t Total Gen Loss : 31.786436080932617, \t Total Dis Loss : 0.000578808132559061\n",
      "Steps : 55100, \t Total Gen Loss : 30.326595306396484, \t Total Dis Loss : 0.00037170990253798664\n",
      "Steps : 55200, \t Total Gen Loss : 31.917985916137695, \t Total Dis Loss : 0.0004405045765452087\n",
      "Steps : 55300, \t Total Gen Loss : 32.13435363769531, \t Total Dis Loss : 0.0002877105725929141\n",
      "Steps : 55400, \t Total Gen Loss : 29.93098258972168, \t Total Dis Loss : 0.0003424307215027511\n",
      "Steps : 55500, \t Total Gen Loss : 31.608356475830078, \t Total Dis Loss : 0.000315572862746194\n",
      "Steps : 55600, \t Total Gen Loss : 31.096988677978516, \t Total Dis Loss : 0.000306081521557644\n",
      "Steps : 55700, \t Total Gen Loss : 29.509742736816406, \t Total Dis Loss : 0.0011301401536911726\n",
      "Steps : 55800, \t Total Gen Loss : 33.53565979003906, \t Total Dis Loss : 0.0008717029122635722\n",
      "Steps : 55900, \t Total Gen Loss : 30.309036254882812, \t Total Dis Loss : 0.0032019554637372494\n",
      "Steps : 56000, \t Total Gen Loss : 33.18580627441406, \t Total Dis Loss : 9.251077426597476e-05\n",
      "Steps : 56100, \t Total Gen Loss : 33.2158317565918, \t Total Dis Loss : 0.0003492406103760004\n",
      "Steps : 56200, \t Total Gen Loss : 33.46184158325195, \t Total Dis Loss : 0.00023395358584821224\n",
      "Steps : 56300, \t Total Gen Loss : 33.15160369873047, \t Total Dis Loss : 0.00017052527982741594\n",
      "Steps : 56400, \t Total Gen Loss : 36.4820442199707, \t Total Dis Loss : 0.0034747356548905373\n",
      "Steps : 56500, \t Total Gen Loss : 35.009483337402344, \t Total Dis Loss : 5.9684301959350705e-05\n",
      "Steps : 56600, \t Total Gen Loss : 31.384300231933594, \t Total Dis Loss : 0.0004629381000995636\n",
      "Steps : 56700, \t Total Gen Loss : 31.90233039855957, \t Total Dis Loss : 0.0005199587321840227\n",
      "Steps : 56800, \t Total Gen Loss : 31.06179428100586, \t Total Dis Loss : 0.0006064585177227855\n",
      "Steps : 56900, \t Total Gen Loss : 29.103971481323242, \t Total Dis Loss : 0.00034430628875270486\n",
      "Steps : 57000, \t Total Gen Loss : 30.60901641845703, \t Total Dis Loss : 0.0017365401145070791\n",
      "Steps : 57100, \t Total Gen Loss : 33.19071578979492, \t Total Dis Loss : 1.9166613817214966\n",
      "Steps : 57200, \t Total Gen Loss : 32.9764404296875, \t Total Dis Loss : 0.0002723477373365313\n",
      "Steps : 57300, \t Total Gen Loss : 32.43403244018555, \t Total Dis Loss : 0.0004783382755704224\n",
      "Steps : 57400, \t Total Gen Loss : 32.82741165161133, \t Total Dis Loss : 0.00020890192536171526\n",
      "Steps : 57500, \t Total Gen Loss : 31.378093719482422, \t Total Dis Loss : 0.0001617094676475972\n",
      "Steps : 57600, \t Total Gen Loss : 31.794368743896484, \t Total Dis Loss : 0.00016319916176144034\n",
      "Steps : 57700, \t Total Gen Loss : 30.562135696411133, \t Total Dis Loss : 0.0006423307931981981\n",
      "Steps : 57800, \t Total Gen Loss : 33.4268684387207, \t Total Dis Loss : 0.0004120732191950083\n",
      "Steps : 57900, \t Total Gen Loss : 32.43210220336914, \t Total Dis Loss : 0.00047810361138544977\n",
      "Steps : 58000, \t Total Gen Loss : 31.325862884521484, \t Total Dis Loss : 0.0003753632481675595\n",
      "Steps : 58100, \t Total Gen Loss : 33.188602447509766, \t Total Dis Loss : 0.00024463824229314923\n",
      "Steps : 58200, \t Total Gen Loss : 32.774017333984375, \t Total Dis Loss : 0.00018565033678896725\n",
      "Steps : 58300, \t Total Gen Loss : 32.585357666015625, \t Total Dis Loss : 0.00031858737929724157\n",
      "Steps : 58400, \t Total Gen Loss : 32.8303108215332, \t Total Dis Loss : 0.00011352644651196897\n",
      "Steps : 58500, \t Total Gen Loss : 33.830108642578125, \t Total Dis Loss : 0.00048178256838582456\n",
      "Steps : 58600, \t Total Gen Loss : 30.721927642822266, \t Total Dis Loss : 0.0016419661697000265\n",
      "Steps : 58700, \t Total Gen Loss : 34.64104461669922, \t Total Dis Loss : 0.00023131318448577076\n",
      "Steps : 58800, \t Total Gen Loss : 35.16084289550781, \t Total Dis Loss : 0.0001818838354665786\n",
      "Steps : 58900, \t Total Gen Loss : 32.14785385131836, \t Total Dis Loss : 0.0020864943508058786\n",
      "Steps : 59000, \t Total Gen Loss : 32.1593017578125, \t Total Dis Loss : 0.00018603896023705602\n",
      "Steps : 59100, \t Total Gen Loss : 32.639442443847656, \t Total Dis Loss : 0.0003242495295125991\n",
      "Steps : 59200, \t Total Gen Loss : 33.139678955078125, \t Total Dis Loss : 0.0003104492789134383\n",
      "Steps : 59300, \t Total Gen Loss : 32.60478973388672, \t Total Dis Loss : 0.00033667098614387214\n",
      "Steps : 59400, \t Total Gen Loss : 32.57425308227539, \t Total Dis Loss : 0.0006564480136148632\n",
      "Steps : 59500, \t Total Gen Loss : 32.55067443847656, \t Total Dis Loss : 0.0003619551134761423\n",
      "Steps : 59600, \t Total Gen Loss : 32.05672836303711, \t Total Dis Loss : 0.000835643382743001\n",
      "Steps : 59700, \t Total Gen Loss : 34.50220489501953, \t Total Dis Loss : 0.00014234402624424547\n",
      "Steps : 59800, \t Total Gen Loss : 33.006690979003906, \t Total Dis Loss : 0.0007980148075148463\n",
      "Steps : 59900, \t Total Gen Loss : 32.90373611450195, \t Total Dis Loss : 0.0005442063556984067\n",
      "Steps : 60000, \t Total Gen Loss : 31.905391693115234, \t Total Dis Loss : 0.0005417597130872309\n",
      "Steps : 60100, \t Total Gen Loss : 33.17109680175781, \t Total Dis Loss : 0.00014091224875301123\n",
      "Steps : 60200, \t Total Gen Loss : 32.32858657836914, \t Total Dis Loss : 0.0003929612576030195\n",
      "Steps : 60300, \t Total Gen Loss : 32.98646545410156, \t Total Dis Loss : 0.00016908551333472133\n",
      "Steps : 60400, \t Total Gen Loss : 32.209285736083984, \t Total Dis Loss : 0.00012470479123294353\n",
      "Steps : 60500, \t Total Gen Loss : 32.027069091796875, \t Total Dis Loss : 0.0008027820731513202\n",
      "Steps : 60600, \t Total Gen Loss : 31.892749786376953, \t Total Dis Loss : 0.0010151563910767436\n",
      "Steps : 60700, \t Total Gen Loss : 36.701759338378906, \t Total Dis Loss : 0.000166666490258649\n",
      "Time for epoch 9 is 541.4915578365326 sec\n",
      "Steps : 60800, \t Total Gen Loss : 36.165557861328125, \t Total Dis Loss : 0.00018084782641381025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 60900, \t Total Gen Loss : 34.395938873291016, \t Total Dis Loss : 0.0002197232679463923\n",
      "Steps : 61000, \t Total Gen Loss : 33.1950798034668, \t Total Dis Loss : 0.0002660469035618007\n",
      "Steps : 61100, \t Total Gen Loss : 32.84169387817383, \t Total Dis Loss : 0.0004048227274324745\n",
      "Steps : 61200, \t Total Gen Loss : 36.194061279296875, \t Total Dis Loss : 0.0030915781389921904\n",
      "Steps : 61300, \t Total Gen Loss : 33.076751708984375, \t Total Dis Loss : 0.000258435815339908\n",
      "Steps : 61400, \t Total Gen Loss : 31.651229858398438, \t Total Dis Loss : 0.00025839172303676605\n",
      "Steps : 61500, \t Total Gen Loss : 33.43635177612305, \t Total Dis Loss : 0.0002446325379423797\n",
      "Steps : 61600, \t Total Gen Loss : 33.887062072753906, \t Total Dis Loss : 0.00026441464433446527\n",
      "Steps : 61700, \t Total Gen Loss : 36.24104309082031, \t Total Dis Loss : 0.006903739180415869\n",
      "Steps : 61800, \t Total Gen Loss : 33.688785552978516, \t Total Dis Loss : 0.001027360325679183\n",
      "Steps : 61900, \t Total Gen Loss : 32.84803009033203, \t Total Dis Loss : 0.0002936954260803759\n",
      "Steps : 62000, \t Total Gen Loss : 32.026702880859375, \t Total Dis Loss : 0.00027975713601335883\n",
      "Steps : 62100, \t Total Gen Loss : 34.55354690551758, \t Total Dis Loss : 0.0009790733456611633\n",
      "Steps : 62200, \t Total Gen Loss : 32.98688507080078, \t Total Dis Loss : 0.0007727051270194352\n",
      "Steps : 62300, \t Total Gen Loss : 33.58855056762695, \t Total Dis Loss : 0.0007155092316679657\n",
      "Steps : 62400, \t Total Gen Loss : 34.7136116027832, \t Total Dis Loss : 7.949675637064502e-05\n",
      "Steps : 62500, \t Total Gen Loss : 35.38107681274414, \t Total Dis Loss : 0.00013033323921263218\n",
      "Steps : 62600, \t Total Gen Loss : 37.748016357421875, \t Total Dis Loss : 0.0015015833778306842\n",
      "Steps : 62700, \t Total Gen Loss : 36.20619201660156, \t Total Dis Loss : 0.00039465446025133133\n",
      "Steps : 62800, \t Total Gen Loss : 38.351463317871094, \t Total Dis Loss : 0.004308112431317568\n",
      "Steps : 62900, \t Total Gen Loss : 33.659908294677734, \t Total Dis Loss : 0.00044805731158703566\n",
      "Steps : 63000, \t Total Gen Loss : 33.18756103515625, \t Total Dis Loss : 0.0008284150389954448\n",
      "Steps : 63100, \t Total Gen Loss : 33.44685363769531, \t Total Dis Loss : 0.00012437737314030528\n",
      "Steps : 63200, \t Total Gen Loss : 31.9270076751709, \t Total Dis Loss : 0.0007109151920303702\n",
      "Steps : 63300, \t Total Gen Loss : 32.628414154052734, \t Total Dis Loss : 0.0005074741784483194\n",
      "Steps : 63400, \t Total Gen Loss : 33.97439956665039, \t Total Dis Loss : 0.00012449595669750124\n",
      "Steps : 63500, \t Total Gen Loss : 33.394901275634766, \t Total Dis Loss : 0.00013895165466237813\n",
      "Steps : 63600, \t Total Gen Loss : 34.135841369628906, \t Total Dis Loss : 0.00010252212814521044\n",
      "Steps : 63700, \t Total Gen Loss : 33.8140983581543, \t Total Dis Loss : 0.0014251482207328081\n",
      "Steps : 63800, \t Total Gen Loss : 34.460243225097656, \t Total Dis Loss : 2.92892746074358e-05\n",
      "Steps : 63900, \t Total Gen Loss : 31.484373092651367, \t Total Dis Loss : 0.0006332919001579285\n",
      "Steps : 64000, \t Total Gen Loss : 29.965652465820312, \t Total Dis Loss : 0.0005567833431996405\n",
      "Steps : 64100, \t Total Gen Loss : 35.192420959472656, \t Total Dis Loss : 0.00022884363716002554\n",
      "Steps : 64200, \t Total Gen Loss : 32.679344177246094, \t Total Dis Loss : 0.00033832559711299837\n",
      "Steps : 64300, \t Total Gen Loss : 32.473636627197266, \t Total Dis Loss : 0.00020981053239665926\n",
      "Steps : 64400, \t Total Gen Loss : 35.327552795410156, \t Total Dis Loss : 0.00011203184840269387\n",
      "Steps : 64500, \t Total Gen Loss : 30.51351547241211, \t Total Dis Loss : 0.0027378760278224945\n",
      "Steps : 64600, \t Total Gen Loss : 31.61379623413086, \t Total Dis Loss : 0.002532815095037222\n",
      "Steps : 64700, \t Total Gen Loss : 33.95347595214844, \t Total Dis Loss : 0.00022066451492719352\n",
      "Steps : 64800, \t Total Gen Loss : 35.95772171020508, \t Total Dis Loss : 7.613080379087478e-05\n",
      "Steps : 64900, \t Total Gen Loss : 32.30662155151367, \t Total Dis Loss : 0.0007795955170877278\n",
      "Steps : 65000, \t Total Gen Loss : 32.11085510253906, \t Total Dis Loss : 0.0005570139037445188\n",
      "Steps : 65100, \t Total Gen Loss : 32.498592376708984, \t Total Dis Loss : 0.0003220227372366935\n",
      "Steps : 65200, \t Total Gen Loss : 31.965473175048828, \t Total Dis Loss : 0.0002964687009807676\n",
      "Steps : 65300, \t Total Gen Loss : 32.478267669677734, \t Total Dis Loss : 0.00229114992544055\n",
      "Steps : 65400, \t Total Gen Loss : 31.61756706237793, \t Total Dis Loss : 0.12323740124702454\n",
      "Steps : 65500, \t Total Gen Loss : 32.951385498046875, \t Total Dis Loss : 0.00056600256357342\n",
      "Steps : 65600, \t Total Gen Loss : 30.455886840820312, \t Total Dis Loss : 0.00021134965936653316\n",
      "Steps : 65700, \t Total Gen Loss : 30.73749351501465, \t Total Dis Loss : 0.0004323520406614989\n",
      "Steps : 65800, \t Total Gen Loss : 28.661340713500977, \t Total Dis Loss : 0.013084905222058296\n",
      "Steps : 65900, \t Total Gen Loss : 32.12690734863281, \t Total Dis Loss : 0.001937149092555046\n",
      "Steps : 66000, \t Total Gen Loss : 34.00800704956055, \t Total Dis Loss : 0.0016451220726594329\n",
      "Steps : 66100, \t Total Gen Loss : 34.0582389831543, \t Total Dis Loss : 0.00012096069258404896\n",
      "Steps : 66200, \t Total Gen Loss : 35.08386993408203, \t Total Dis Loss : 0.0011227249633520842\n",
      "Steps : 66300, \t Total Gen Loss : 33.06776428222656, \t Total Dis Loss : 0.5500057339668274\n",
      "Steps : 66400, \t Total Gen Loss : 32.89895248413086, \t Total Dis Loss : 0.0001535540068289265\n",
      "Steps : 66500, \t Total Gen Loss : 35.28647232055664, \t Total Dis Loss : 0.00630268594250083\n",
      "Steps : 66600, \t Total Gen Loss : 31.82022476196289, \t Total Dis Loss : 0.0007761125452816486\n",
      "Steps : 66700, \t Total Gen Loss : 32.77794647216797, \t Total Dis Loss : 0.0002127843035850674\n",
      "Steps : 66800, \t Total Gen Loss : 33.9075927734375, \t Total Dis Loss : 0.00034033635165542364\n",
      "Steps : 66900, \t Total Gen Loss : 32.19355773925781, \t Total Dis Loss : 0.0005736922030337155\n",
      "Steps : 67000, \t Total Gen Loss : 36.55409622192383, \t Total Dis Loss : 0.006396921817213297\n",
      "Steps : 67100, \t Total Gen Loss : 32.81505584716797, \t Total Dis Loss : 0.0006381424609571695\n",
      "Steps : 67200, \t Total Gen Loss : 35.335777282714844, \t Total Dis Loss : 0.0008127349428832531\n",
      "Steps : 67300, \t Total Gen Loss : 35.335933685302734, \t Total Dis Loss : 0.0007701771100983024\n",
      "Steps : 67400, \t Total Gen Loss : 33.867958068847656, \t Total Dis Loss : 0.00015107891522347927\n",
      "Steps : 67500, \t Total Gen Loss : 35.16204833984375, \t Total Dis Loss : 0.0004918609047308564\n",
      "Time for epoch 10 is 547.3103680610657 sec\n",
      "Steps : 67600, \t Total Gen Loss : 35.869869232177734, \t Total Dis Loss : 0.002292441437020898\n",
      "Steps : 67700, \t Total Gen Loss : 33.554927825927734, \t Total Dis Loss : 0.0003326953446958214\n",
      "Steps : 67800, \t Total Gen Loss : 33.73326873779297, \t Total Dis Loss : 0.00012612628052011132\n",
      "Steps : 67900, \t Total Gen Loss : 31.061222076416016, \t Total Dis Loss : 0.00017467164434492588\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)#수정....\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T12:45:31.430321Z",
     "start_time": "2020-11-18T12:45:31.423185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f6f0e28bcd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T12:45:33.122735Z",
     "start_time": "2020-11-18T12:45:33.115977Z"
    }
   },
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T12:46:27.259004Z",
     "start_time": "2020-11-18T12:45:33.983010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T12:46:27.299739Z",
     "start_time": "2020-11-18T12:46:27.285159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T13:22:56.769264Z",
     "start_time": "2020-11-18T13:22:56.357083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T13:22:57.449939Z",
     "start_time": "2020-11-18T13:22:57.299425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5050549 0.48207346\n",
      "0.1276761 0.12728648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T13:22:58.804936Z",
     "start_time": "2020-11-18T13:22:58.800823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
