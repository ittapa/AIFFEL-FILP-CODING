{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빅 데이터 에코 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T07:04:44.252277Z",
     "start_time": "2020-09-21T07:04:44.250573Z"
    }
   },
   "source": [
    "## 연대기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T07:05:44.810304Z",
     "start_time": "2020-09-21T07:05:44.803379Z"
    }
   },
   "source": [
    "### 2003년: GFS(Google File System)\n",
    "- IT업계에서 매우 유명한 회사인 구글은 검색엔진 뿐만 아니라 Youtube, AI, 클라우드, 자율주행 자동차 등 정말 많은 Business하는 회사 입니다. 사실 구글은 새로운 데이터에 대한 필요성(Needs) 그 변화의 바람을 먼저 읽고 준비한 회사라고 할 수 있습니다. 그만큼 초기부터 데이터에 대한 고민을 정말 많이하였고 새로운 데이터 시대에 맞는 피일시스템과 프로그래밍 모델을 각각 발표하는데요, 이는 현재 빅데이터의 근간이 되는 기술입니다. 우선 제일 처음으로 2003년에는 빅데이터용의 새로운 파일 시스템인 GFS(Google File System)을 발표합니다. 시스템 고장을 효과적으로 처리할 수 있게 복제가 굉장히 용이하고 분산처리에 적합한 파일 시스템 입니다.\n",
    "\n",
    "\n",
    "### 2004년: MapReduce on Simplified Data Processing on Large Clusters\n",
    "- 뒤이어 바로 1년 뒤인 2004년 구글에서는 분산 처리 환경에 맞는 프로그래밍 모델인 맵리듀스를 발표 합니다. GFS와 같은 분산 처리 파일 시스템에 적용하기 쉬운 프로그래밍 모델입니다. 데이터관련 작업을 맵(map) 함수와 리듀스(reduce) 함수의 2가지 작업으로 나누어 처리하는 것으로, 맵 함수에서는 키-값 쌍을 처리해 중간의 키-값 쌍을 생성하고 리듀스 함수는 동일한 키와 연관된 모든 중간의 값들을 병합하는 함수 였습니다.\n",
    "\n",
    "### 2004년 - 2005년: NDFS Project\n",
    "- 아파치(Apache) 재단의 더그 커팅(Doug Cutting)과 마이크 카파렐라(Mike Cafarella)가 중심이 되어 검색엔진의 효과적인 분산처리를 위해 NDFS(Nutch Distributed File System) 이란 프로젝트를 시작합니다. Nutch(너치)는 아마 들어 보신 분도 계실텐데요 엘라스틱 서치라고 하는 검색엔진의 전신 소프트웨어 입니다.\n",
    "\n",
    "### 2006년 - 2007년: Apache Hadoop\n",
    "- 그리고 더그 커팅(Doug Cutting)이 중심이 되어 중심이 되어 구글이 발표한 맵리듀스와 GFS개념을 더 보완하여 빅데이터용 오픈소스 프로젝트를 시작하는데요, 하둡이 어느 정도 성과를 보이자 아파치 재단에서는 이 프로젝트를 가장 우선순위가 높은 프로젝트로 공식 프로젝트를 발표하고, 2006년 하둡 1.0 이 만들어 집니다. 하둡 1.0의 핵심 기술은 HDFS(Hadoop Distributed File System)과 (하둡)MapReduce 2가지 입니다. 현재 빅데이터에 근간이 되는 기술들 이에요. (공식 발표는 2007년 입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T07:04:36.230839Z",
     "start_time": "2020-09-21T07:04:36.015084Z"
    }
   },
   "source": [
    "## Hadoop Ecosystem\n",
    "![](imgs/01.png)\n",
    "![](imgs/02.png)\n",
    "\n",
    "### 데이터 수집(Data Ingestion)\n",
    "- 스쿱(Sqoop) : RDBMS(오라클, MySQL등..)와 하둡 사이의 데이터를 이동시켜줍니다.\n",
    "\n",
    "- 플럼(Flume) : 분산환경에서 대량의 로그데이터를 효과적으로 수집하여 합친 후 다른 곳으로 전송 합니다.\n",
    "\n",
    "### 데이터 처리(Data Processing)\n",
    "- 하둡 분산파일시스템(HDFS): 하둡의 분산 처리 파일 시스템\n",
    "\n",
    "- 맵리듀스(MapReduce): Java기반의 맵리듀스 프로그래밍 모델 입니다.\n",
    "\n",
    "- 얀(Yarn): 하둡 클러스터의 자원(Resource)를 관리\n",
    "\n",
    "- 스파크(Spark): In-memory기반의 클러스터 컴퓨팅 데이터 처리 입니다. 스파크 안에도 스파크 코어, 스파크SQ, Milib, GraphX과 같은 컴포넌트가 있습니다.\n",
    "\n",
    "### 데이터 분석(Data Analysis)\n",
    "- 피그(Pig): 맵리듀스로 실행하기 어려운 데이터 관련 작업, filter, join, query와 같은 작업을 실행 합니다.\n",
    "\n",
    "- 임팔라(Impala): 고성능의 SQL 엔진\n",
    "\n",
    "- 하이브(Hive): 임팔라와 유사한 SQL 관련 기능을 제공합니다.\n",
    "\n",
    "### 데이터 검색(Data Exploration)\n",
    " - 클라우데라 서치(Cloudera Search): real-time으로 데이터에 검색이 가능합니다.\n",
    "\n",
    "휴(Hue): 웹 인터페이스 제공\n",
    "\n",
    "### 기타\n",
    "- 우지(Oozie): 워크플로우 관리, Job 스케쥴러\n",
    "\n",
    "- HBase: NoSQL기반으로 HDFS에 의해 처리된 데이터를 저장합니다.\n",
    "\n",
    "- 제플린(Zeppelin): 데이터 시각화\n",
    "\n",
    "- SparkMLlib, 머하웃(mahout): 머신러닝 관련 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Ecosystem\n",
    "\n",
    "![](imgs/03.png)\n",
    "\n",
    "- Spark SQL: SQL 관련 작업\n",
    "- Streaming: Streaming 데이터 처리\n",
    "- MLlib: Machine Learning 관련 라이브러리\n",
    "- GraphX: Graph Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T07:36:45.023856Z",
     "start_time": "2020-09-21T07:36:45.022387Z"
    }
   },
   "source": [
    "### RDD\n",
    "- resilient distributed data\n",
    "- In-Memory\n",
    "- Fault Tolerance\n",
    "- Immutable(Read-Only)\n",
    "- Partition [파티션]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:08:29.838128Z",
     "start_time": "2020-09-22T02:08:29.835325Z"
    }
   },
   "source": [
    "#### rdd creation\n",
    "- 내부에서 만들어진 데이터 집합을 병렬화 하는 방법: parallelize()함수 사용\n",
    "- 외부의 파일을 로드하는 방법: .textFile() 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:09:43.084374Z",
     "start_time": "2020-09-22T02:09:43.036898Z"
    }
   },
   "source": [
    "#### rdd operation\n",
    "- Transformations :RDD에게 변형방법(연산로직, 계보, lineage)을 알려주고 새로운 RDD를 만듭\n",
    "- Actions : 실제 연산 수행\n",
    "![](./imgs/04.png)\n",
    "![](./imgs/05.png)\n",
    "![](./imgs/06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:20:09.931952Z",
     "start_time": "2020-09-22T02:20:09.930302Z"
    }
   },
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:20:32.999059Z",
     "start_time": "2020-09-22T02:20:32.778619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.8\" 2020-07-14\r\n",
      "OpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1)\r\n",
      "OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1, mixed mode, sharing)\r\n"
     ]
    }
   ],
   "source": [
    "# java 버전 확인\n",
    "! java -version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 자바가 설치 안되 있으면 설치\n",
    "#! sudo apt-get install openjdk-8-jdk-headless -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:23:37.292738Z",
     "start_time": "2020-09-22T02:21:24.645552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-22 11:21:24--  http://mirror.apache-kr.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
      "Resolving mirror.apache-kr.org (mirror.apache-kr.org)... 125.209.216.167\n",
      "접속 mirror.apache-kr.org (mirror.apache-kr.org)|125.209.216.167|:80... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 219929956 (210M) [application/octet-stream]\n",
      "Saving to: ‘spark-3.0.1-bin-hadoop2.7.tgz’\n",
      "\n",
      "spark-3.0.1-bin-had 100%[===================>] 209.74M  2.12MB/s    in 2m 12s  \n",
      "\n",
      "2020-09-22 11:23:37 (1.59 MB/s) - ‘spark-3.0.1-bin-hadoop2.7.tgz’ saved [219929956/219929956]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spakrk 다운로드\n",
    "#!wget http://mirror.apache-kr.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:36:37.963788Z",
     "start_time": "2020-09-22T02:33:03.714647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/09/22 11:33:04 WARN Utils: Your hostname, aiffel0042-LAPTOP resolves to a loopback address: 127.0.1.1; using 192.168.1.105 instead (on interface wlo1)\n",
      "20/09/22 11:33:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/aiffel0042/aiffel/AIFFEL_LSG/utill/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "20/09/22 11:33:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://192.168.1.105:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1600741989609).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.8)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\u001b[35m\n",
      "scala> \u001b[0m\n",
      "\u001b[35m\n",
      "scala> \u001b[0m"
     ]
    }
   ],
   "source": [
    "# shell 실행\n",
    "#! /home/aiffel0042/aiffel/AIFFEL_LSG/utill/spark-3.0.1-bin-hadoop2.7/bin/spark-shell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:37:21.274621Z",
     "start_time": "2020-09-22T02:36:40.612092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2 MB 10.9 MB/s eta 0:00:01    |███████████████████▉            | 126.6 MB 5.1 MB/s eta 0:00:16\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=2bdf6dcb0b18e36fbd5ef1d34de0aef21c2570d3ffee26e8fdbbfd2175a1b029\n",
      "  Stored in directory: /home/aiffel0042/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
     ]
    }
   ],
   "source": [
    "## pyspark version check\n",
    "#! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:51:50.697503Z",
     "start_time": "2020-09-22T02:51:50.667182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pyspark version check\n",
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SparkContext 객체\n",
    "- 트리포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:59:47.241213Z",
     "start_time": "2020-09-22T02:59:47.145442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.105:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# context define\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:59:48.681643Z",
     "start_time": "2020-09-22T02:59:48.671697Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-17-f74d55668885>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8f1079901140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#에러가 발생함을 확인하세요!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-17-f74d55668885>:5 "
     ]
    }
   ],
   "source": [
    "#에러가 발생함을 확인하세요!\n",
    "new_sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:00:12.259704Z",
     "start_time": "2020-09-22T03:00:12.231684Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:00:13.596187Z",
     "start_time": "2020-09-22T03:00:13.515232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.105:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuration \n",
    "sc = SparkContext(master='local', appName='PySpark Basic')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:00:17.957481Z",
     "start_time": "2020-09-22T03:00:17.945167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1600743613582'),\n",
       " ('spark.app.name', 'PySpark Basic'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.1.105'),\n",
       " ('spark.driver.port', '43053'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:00:22.289786Z",
     "start_time": "2020-09-22T03:00:22.287485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:00:26.274724Z",
     "start_time": "2020-09-22T03:00:26.272396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark Basic'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:00:32.609585Z",
     "start_time": "2020-09-22T03:00:32.466636Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:20:09.712831Z",
     "start_time": "2020-09-22T05:20:07.460784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName('PySpark Basic').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:19:51.897167Z",
     "start_time": "2020-09-22T05:19:51.895667Z"
    }
   },
   "source": [
    "### parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:20:21.686240Z",
     "start_time": "2020-09-22T05:20:21.503215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:20:33.847615Z",
     "start_time": "2020-09-22T05:20:33.843850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actoin\n",
    "\n",
    "- take \n",
    "![](./imgs/07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:21:02.392334Z",
     "start_time": "2020-09-22T05:21:01.784665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (외부) textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:30:22.100749Z",
     "start_time": "2020-09-22T05:30:22.098981Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/test.txt'\n",
    "with open(file_path, 'w') as f:\n",
    "    for i in range(10):\n",
    "        f.write(str(i)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:30:29.200035Z",
     "start_time": "2020-09-22T05:30:29.063043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.textFile(file_path)\n",
    "print(rdd2)\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:31:51.287183Z",
     "start_time": "2020-09-22T05:31:51.248938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:32:16.726858Z",
     "start_time": "2020-09-22T05:32:16.725157Z"
    }
   },
   "source": [
    "### trasnformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:32:19.314686Z",
     "start_time": "2020-09-22T05:32:19.310065Z"
    }
   },
   "source": [
    "- map()\n",
    "- filter()\n",
    "- flatmap()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:38:04.544669Z",
     "start_time": "2020-09-22T05:38:04.543145Z"
    }
   },
   "source": [
    "#### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:34:47.022456Z",
     "start_time": "2020-09-22T05:34:46.940059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c', 'd']\n",
      "[('b', 1), ('a', 1), ('c', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([\"b\", \"a\", \"c\", \"d\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect()) #collect()는 actions입니다. \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:34:54.715802Z",
     "start_time": "2020-09-22T05:34:54.680076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3])\n",
    "squares = nums.map(lambda x: x*x)\n",
    "print(squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:42:04.180738Z",
     "start_time": "2020-09-22T05:42:04.115792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.filter(lambda x: x%2 == 0) \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:42:09.460036Z",
     "start_time": "2020-09-22T05:42:09.399628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "text = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "capital = text.map(lambda x: x.upper())\n",
    "A = capital.filter(lambda x: 'A' in x)\n",
    "print(text.collect())\n",
    "print(A.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:54:18.130282Z",
     "start_time": "2020-09-22T05:54:18.086496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 10, 30, 2, 20, 30, 3, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*10, 30))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3개 한방에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:54:23.906744Z",
     "start_time": "2020-09-22T05:54:23.861132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'python']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordsDataset = sc.parallelize([\"Spark is funny\", \"It is beautiful\", \"And also It is implemented by python\"])\n",
    "result = wordsDataset.flatMap(lambda x: x.split()).filter(lambda x: x != \" \").map(lambda x: x.lower())\n",
    "# map 문자나눔\n",
    "# filter 공백 제거 \n",
    "# 소문자(map)\n",
    "# flatMap\n",
    "\n",
    "# 공백은 제거합니다.\n",
    "# 단어를 공백기준으로 split 합니다. \n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 파일 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:56:57.940841Z",
     "start_time": "2020-09-22T05:56:57.431677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-22 14:56:57--  https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 34.64.4.16, 34.64.4.80, 34.64.4.112, ...\n",
      "접속 storage.googleapis.com (storage.googleapis.com)|34.64.4.16|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30874 (30K) [application/octet-stream]\n",
      "Saving to: ‘train.csv’\n",
      "\n",
      "train.csv           100%[===================>]  30.15K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2020-09-22 14:56:57 (3.23 MB/s) - ‘train.csv’ saved [30874/30874]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예제 다운\n",
    "#! wget https://storage.googleapis.com/tf-datasets/titanic/train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T05:57:28.549191Z",
     "start_time": "2020-09-22T05:57:28.487165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone',\n",
       " '0,male,22.0,1,0,7.25,Third,unknown,Southampton,n',\n",
       " '1,female,38.0,1,0,71.2833,First,C,Cherbourg,n',\n",
       " '1,female,26.0,0,0,7.925,Third,unknown,Southampton,y',\n",
       " '1,female,35.0,1,0,53.1,First,C,Southampton,n']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read csv\n",
    "\n",
    "import os\n",
    "csv_path = os.getenv('HOME')+'/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_0.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:03:52.067511Z",
     "start_time": "2020-09-22T06:03:52.025423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비어있는 라인은 제외하고, delimeter인 ,로 line을 분리해 줍니다. \n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "csv_data_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:19:33.774068Z",
     "start_time": "2020-09-22T06:19:33.748994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = csv_data_1.take(1)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:19:52.231381Z",
     "start_time": "2020-09-22T06:19:52.200461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '28.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8.4583',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Queenstown',\n",
       "  'y']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())  # 첫번째 컬럼이 숫자인 것만 필터링\n",
    "csv_data_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:19:57.776020Z",
     "start_time": "2020-09-22T06:19:57.739673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '35.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '53.1'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '28.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '8.4583'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Queenstown'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "csv_data_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:20:11.844947Z",
     "start_time": "2020-09-22T06:20:08.811847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class|deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|0       |male  |22.0|1                 |0    |7.25   |Third|unknown|Southampton|n    |\n",
      "|1       |female|38.0|1                 |0    |71.2833|First|C      |Cherbourg  |n    |\n",
      "|1       |female|26.0|0                 |0    |7.925  |Third|unknown|Southampton|y    |\n",
      "|1       |female|35.0|1                 |0    |53.1   |First|C      |Southampton|n    |\n",
      "|0       |male  |28.0|0                 |0    |8.4583 |Third|unknown|Queenstown |y    |\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "url = 'https://storage.googleapis.com/tf-datasets/titanic/train.csv'\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- collect()\n",
    "- take()\n",
    "- count()\n",
    "- reduce()\n",
    "- saveAsTextFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:35:26.559017Z",
     "start_time": "2020-09-22T06:35:26.532166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:35:51.451063Z",
     "start_time": "2020-09-22T06:35:51.430301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize(list(range(10)))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:36:06.036902Z",
     "start_time": "2020-09-22T06:36:06.008454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:36:19.490729Z",
     "start_time": "2020-09-22T06:36:19.455081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:36:30.154507Z",
     "start_time": "2020-09-22T06:36:30.152775Z"
    }
   },
   "source": [
    "#### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:36:37.755495Z",
     "start_time": "2020-09-22T06:36:37.731067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:36:54.239863Z",
     "start_time": "2020-09-22T06:36:54.238289Z"
    }
   },
   "source": [
    "#### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T07:26:37.611305Z",
     "start_time": "2020-09-22T07:26:37.590054Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o539.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/aiffel0042/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/file.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c85b88b416f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# !ls -l ~/aiffel/bigdata_ecosystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o539.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/aiffel0042/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/file.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')\\\n",
    "+'/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/file.txt'\n",
    "\n",
    "\n",
    "nums.saveAsTextFile(file_path)\n",
    "\n",
    "# !ls -l ~/aiffel/bigdata_ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T06:37:38.016599Z",
     "start_time": "2020-09-22T06:37:37.970766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### summaray\n",
    "# RDD 생성\n",
    "rdd = sc.parallelize(range(1,100))\n",
    "\n",
    "# RDD Transformation \n",
    "rdd2 = rdd.map(lambda x: 0.5*x - 10).filter(lambda x: x > 0)\n",
    "\n",
    "# RDD Action \n",
    "rdd2.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operation (3) 실습:MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T07:07:08.917838Z",
     "start_time": "2020-09-22T07:07:08.649272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 2),\n",
       " ('e', 1),\n",
       " ('l', 2),\n",
       " ('o', 2),\n",
       " ('p', 1),\n",
       " ('y', 1),\n",
       " ('t', 1),\n",
       " ('n', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.parallelize('hello python')\n",
    "\n",
    "# map 함수를 적용한 RDD 구하기\n",
    "text_1 = text.filter(lambda x: x != \" \")\n",
    "text_2 = text_1.map(lambda x:(x, 1))\n",
    "\n",
    "#reduceByKey 함수를 적용한 Word Counter 출력\n",
    "word_count = text_2.reduceByKey(lambda accum, n: accum + n)  \n",
    "word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T07:44:08.922785Z",
     "start_time": "2020-09-22T07:44:08.919776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/home/aiffel0042/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/train.csv MapPartitionsRDD[67] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T07:52:46.629461Z",
     "start_time": "2020-09-22T07:52:46.626000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[71] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T07:07:39.287244Z",
     "start_time": "2020-09-22T07:07:39.207321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 스텝에서 CSV 파일을 로딩했던 내역입니다. \n",
    "# 타이ㄴ타닉 예제임\n",
    "# map 함수를 이용해 모든 데이터를 (생존여부, 연령)의 형태로 바꾸\n",
    "\n",
    "csv_path = os.getenv('HOME')\\\n",
    "+'/aiffel/AIFFEL_LSG/lms/02-FUNDAMENTAL_1-40/F39-bigdata_echo_system/train.csv'\n",
    "\n",
    "# 외부 파일 리드\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "\n",
    "# line 크기가 1 보다 큰 line만 \n",
    "# 콤마  기준으로 분리\n",
    "#  \n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "\n",
    "# take rdd에서 data 컬럼 가져옴\n",
    "columns = csv_data_1.take(1) \n",
    "\n",
    "# 첫번째 , survived 가 십진수 , true, false\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())\n",
    "\n",
    "# \n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "\n",
    "csv_data_3.take(3) # 2번째 값 출력..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T07:08:11.781804Z",
     "start_time": "2020-09-22T07:08:11.615210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생존자 평균 연령: 29.110411522633743\n",
      "사망자 평균 연령: 29.9609375\n"
     ]
    }
   ],
   "source": [
    "csv_data_4 = csv_data_3.map(lambda line:(line[0][1], line[2][1]))   # (생존여부, 연령)\n",
    "age_sum_data = csv_data_4.reduceByKey(lambda accum, age: float(accum) + float(age))  \n",
    "age_sum = age_sum_data.collect()\n",
    "\n",
    "# 생존자와 사망자의 사람 수 구하기\n",
    "csv_data_5 = csv_data_3.map(lambda line:(line[0][1], 1))\n",
    "survived_data = csv_data_5.reduceByKey(lambda accum, count: int(accum) + int(count)) \n",
    "survived_count = survived_data.collect()\n",
    "\n",
    "age_sum_dict = dict(age_sum)\n",
    "survived_dict = dict(survived_count)\n",
    "avg_age_survived = age_sum_dict['1']/survived_dict['1']\n",
    "print('생존자 평균 연령:' ,avg_age_survived)\n",
    "avg_age_died = age_sum_dict['0']/survived_dict['0']\n",
    "print('사망자 평균 연령:' ,avg_age_died)# csv_data_3를 가공하여 생존자, 사망자의 연령 총합과 사람 수를 각각 구해 봅시다. \n",
    "# 이후 각각의 데이터로부터 생존자와 사망자의 평균 연령을 구할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
